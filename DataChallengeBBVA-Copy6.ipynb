{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cargo\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "np.set_printoptions(edgeitems=3,infstr='inf',linewidth=75, nanstr='nan', precision=8,suppress=False, threshold=1000, formatter=None)\n",
    "np.random.seed(2)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print (\"cargo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def datos():\n",
    "    \n",
    "    print(\"Comenzo la carga...\")\n",
    "    train_req = pd.read_excel(\"train_requerimientos.xlsx\")\n",
    "    train_cliente = pd.read_excel(\"train_clientes.xlsx\")\n",
    "    test_cliente = pd.read_excel('test_clientes.xlsx')\n",
    "    test_req = pd.read_excel('test_requerimientos.xlsx')\n",
    "    #'CODMES'\n",
    "\n",
    "    \n",
    "    #Y_train = train_cliente['ATTRITION']\n",
    "    \n",
    "    group_train2 =  train_req.groupby([\"ID_CORRELATIVO\",\"TIPO_REQUERIMIENTO2\",\"DICTAMEN\"])\n",
    "    group_test2 =  test_req.groupby([\"ID_CORRELATIVO\",\"TIPO_REQUERIMIENTO2\",\"DICTAMEN\"])\n",
    "\n",
    "    df_train_req=group_train2.size().to_frame('size').reset_index()\n",
    "    df_test_req=group_test2.size().to_frame('size').reset_index()\n",
    "\n",
    "    df_train_req['FUSION']=df_train_req['TIPO_REQUERIMIENTO2']+\" \"+df_train_req['DICTAMEN']\n",
    "    df_test_req['FUSION']=df_train_req['TIPO_REQUERIMIENTO2']+\" \"+df_train_req['DICTAMEN']\n",
    "\n",
    "    pivot=df_train_req.pivot_table(values='size',index=\"ID_CORRELATIVO\", columns=[\"TIPO_REQUERIMIENTO2\",\"FUSION\"],aggfunc=np.sum)\n",
    "    pivot_test=df_test_req.pivot_table(values='size',index=\"ID_CORRELATIVO\", columns=[\"TIPO_REQUERIMIENTO2\",\"FUSION\"],aggfunc=np.sum)\n",
    "\n",
    "    pivot.fillna(value=0, inplace=True)\n",
    "    pivot_test.fillna(value=0, inplace=True)\n",
    "\n",
    "    pivot_total_reclamo= pivot[\"Reclamo\"].sum(axis=1)\n",
    "    pivot_total_reclamo_test= pivot_test[\"Reclamo\"].sum(axis=1)\n",
    "    #pivot_total_reclamo=pivot_total_reclamo.to_frame('SUMA RECLA')\n",
    "    pivot_total_solicitud= pivot[\"Solicitud\"].sum(axis=1)\n",
    "    pivot_total_solicitud_test= pivot_test[\"Solicitud\"].sum(axis=1)\n",
    "    #pivot_total_solicitud=pivot_total_solicitud.to_frame('SUMA SOLI')\n",
    "    #pivot.reset_index(inplace=True)\n",
    "    #pivot_total_reclamo.reset_index(inplace=True)\n",
    "    #pivot_total_reclamo\n",
    "\n",
    "    pivot_no_procede_reclamo=pivot[\"Reclamo\"][\"Reclamo NO PROCEDE\"]\n",
    "    pivot_reclamo_parcial=pivot[\"Reclamo\"][\"Reclamo PROCEDE PARCIAL\"]\n",
    "\n",
    "    pivot_no_procede_reclamo_test=pivot_test[\"Reclamo\"][\"Reclamo NO PROCEDE\"]\n",
    "    pivot_reclamo_parcial_test=pivot_test[\"Reclamo\"][\"Reclamo PROCEDE PARCIAL\"]\n",
    "    #pivot_no_procede_reclamo.to_frame('No Reclamo')\n",
    "    pivot_no_procede_solicitud=pivot[\"Solicitud\"][\"Solicitud NO PROCEDE\"]\n",
    "    pivot_solicitud_parcial=pivot[\"Solicitud\"][\"Solicitud PROCEDE PARCIAL\"]\n",
    "\n",
    "    pivot_no_procede_solicitud_test=pivot_test[\"Solicitud\"][\"Solicitud NO PROCEDE\"]\n",
    "    pivot_solicitud_parcial_test=pivot_test[\"Solicitud\"][\"Solicitud PROCEDE PARCIAL\"]\n",
    "\n",
    "\n",
    "    #pivot_no_procede_reclamo.to_frame('No Solicitud')\n",
    "\n",
    "    requeremiento={}\n",
    "    requeremiento_test={}\n",
    "    requerimiento={\"reclamos\":pivot_total_reclamo, \"no_reclamos\":pivot_no_procede_reclamo,\"reclamo_parcial\":pivot_reclamo_parcial ,\"solicitudes\":pivot_total_solicitud,\"no_solicitudes\":pivot_no_procede_solicitud,\"solicitudes_parcial\":pivot_solicitud_parcial}\n",
    "    requerimiento_test={\"reclamos\":pivot_total_reclamo_test, \"no_reclamos\":pivot_no_procede_reclamo_test ,\"reclamo_parcial\":pivot_reclamo_parcial_test,\"solicitudes\":pivot_total_solicitud_test,\"no_solicitudes\":pivot_no_procede_solicitud_test,\"solicitudes_parcial\":pivot_solicitud_parcial_test}\n",
    "\n",
    "\n",
    "    df_requerimiento=pd.DataFrame(requerimiento)\n",
    "    df_requerimiento_test=pd.DataFrame(requerimiento_test)\n",
    "\n",
    "    df_requerimiento[\"no_reclamos\"] = np.where(df_requerimiento[\"reclamos\"]==0, 0, df_requerimiento[\"no_reclamos\"]/df_requerimiento[\"reclamos\"])\n",
    "    df_requerimiento[\"reclamo_parcial\"] = np.where(df_requerimiento[\"reclamos\"]==0, 0, df_requerimiento[\"reclamo_parcial\"]/df_requerimiento[\"reclamos\"])\n",
    "\n",
    "    df_requerimiento_test[\"no_reclamos\"] = np.where(df_requerimiento_test[\"reclamos\"]==0, 0, df_requerimiento_test[\"no_reclamos\"]/df_requerimiento_test[\"reclamos\"])\n",
    "    df_requerimiento_test[\"reclamo_parcial\"] = np.where(df_requerimiento_test[\"reclamos\"]==0, 0, df_requerimiento_test[\"reclamo_parcial\"]/df_requerimiento_test[\"reclamos\"])\n",
    "\n",
    "    #procesado=pd.DataFrame(pivot_total_reclamo,pivot_total_solicitud)\n",
    "    df_requerimiento[\"no_solicitudes\"] = np.where(df_requerimiento[\"solicitudes\"]==0, 0, df_requerimiento[\"no_solicitudes\"]/df_requerimiento[\"solicitudes\"])\n",
    "    df_requerimiento[\"solicitudes_parcial\"] = np.where(df_requerimiento[\"solicitudes\"]==0, 0, df_requerimiento[\"solicitudes_parcial\"]/df_requerimiento[\"solicitudes\"])\n",
    "\n",
    "    df_requerimiento_test[\"no_solicitudes\"] = np.where(df_requerimiento_test[\"solicitudes\"]==0, 0, df_requerimiento_test[\"no_solicitudes\"]/df_requerimiento_test[\"solicitudes\"])\n",
    "    df_requerimiento_test[\"solicitudes_parcial\"] = np.where(df_requerimiento_test[\"solicitudes\"]==0, 0, df_requerimiento_test[\"solicitudes_parcial\"]/df_requerimiento_test[\"solicitudes\"])\n",
    "\n",
    "    df_requerimiento.reset_index(inplace=True)\n",
    "    df_requerimiento_test.reset_index(inplace=True)\n",
    "\n",
    "    train=pd.merge(train_cliente,df_requerimiento, on=\"ID_CORRELATIVO\", how='left')\n",
    "    test=pd.merge(test_cliente,df_requerimiento_test, on=\"ID_CORRELATIVO\", how='left')\n",
    "    \n",
    "    values = {'no_reclamos': 0, 'no_solicitudes': 0, 'reclamos': 0,'solicitudes': 0 ,'no_reclamos': 0,'no_solicitudes': 0,'reclamo_parcial':0,'solicitudes_parcial':0}\n",
    "    train.fillna(value=values, inplace=True)\n",
    "    test.fillna(value=values, inplace=True)\n",
    "    print (\"antes\")\n",
    "    print (train.shape)\n",
    "    ##ATTRITION\n",
    "    train.drop(['ID_CORRELATIVO','CODMES'], axis=1, inplace=True)\n",
    "    test.drop(['ID_CORRELATIVO','CODMES'], axis=1, inplace=True)\n",
    "    print (\"despues\")\n",
    "    print (train.shape)\n",
    "\n",
    "    train.dropna(inplace=True)\n",
    "    test.dropna(inplace=True)\n",
    "    \n",
    "    train= pd.get_dummies(train)\n",
    "    \n",
    "    #X_train, Y_train, X_test, Y_test\n",
    "    X_train=train.drop(['ATTRITION'], axis=1)\n",
    "    \n",
    "    Y_train_c=train[['ATTRITION']]\n",
    "    Y_train = Y_train_c.copy()\n",
    "    Y_train['NATTRITION']=1-Y_train['ATTRITION']   \n",
    "    \n",
    "    X_train_p, X_test_p, Y_train_p,Y_test_p = train_test_split(X_train,Y_train,train_size=0.85, random_state=0)\n",
    "    \n",
    "    X_train_p = X_train_p.as_matrix()\n",
    "    Y_train_p = Y_train_p.as_matrix()\n",
    "    X_test_p = X_test_p.as_matrix()\n",
    "    Y_test_p = Y_test_p.as_matrix()\n",
    "    ##normalizar datos de entrada\n",
    "    scaler= MinMaxScaler()\n",
    "    scaler.fit(X_train_p)\n",
    "    \n",
    "    X_train_p = scaler.transform(X_train_p)\n",
    "    X_test_p = scaler.transform(X_test_p)\n",
    "    \n",
    "    print (\"Termino cargar\")\n",
    "    return X_train_p.T, Y_train_p.T, X_test_p.T, Y_test_p.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargo Modelo\n"
     ]
    }
   ],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    X = tf.placeholder(tf.float32,shape=(n_x, None),name=\"X\")\n",
    "    Y = tf.placeholder(tf.float32,shape=(n_y, None),name=\"Y\")\n",
    "    ### Otras variables ###\n",
    "    \n",
    "     # variable learning rate\n",
    "    lr = tf.placeholder(tf.float32)\n",
    "    # train/test selector for batch normalisation\n",
    "    tst = tf.placeholder(tf.bool)\n",
    "    # training iteration\n",
    "    iter = tf.placeholder(tf.int32)   \n",
    "    # training iteration\n",
    "    pkeep = tf.placeholder(tf.float32)      \n",
    "    return X, Y, tst, iter,pkeep\n",
    "\n",
    "def initialize_parameters(layer,tipo=1):\n",
    "    parameters={}\n",
    "    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n",
    "    L=len(layer)\n",
    "    for i in range(1, L):\n",
    "        if(tipo==1):\n",
    "            parameters['W' + str(i)] =tf.get_variable('W' + str(i), [layer[i], layer[i-1]], initializer = tf.contrib.layers.xavier_initializer(seed = 1))        \n",
    "            parameters['b' + str(i)] =tf.get_variable('b' + str(i), [layer[i], 1], initializer = tf.zeros_initializer())\n",
    "        \n",
    "        if(tipo==2):\n",
    "            parameters['W' + str(i)] =tf.Variable(np.random.randn(layer[i], layer[i-1])*0.01,dtype=tf.float32,name='W' + str(i))        \n",
    "            parameters['b' + str(i)] =tf.get_variable('b' + str(i), [layer[i], 1], initializer = tf.zeros_initializer())\n",
    "            \n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def batchnorm(Ylogits, is_test,iter):\n",
    "    Ylogits = tf.transpose(Ylogits)\n",
    "    exp_moving_avg = tf.train.ExponentialMovingAverage(0.999,iter) # adding the iteration prevents from averaging across non-existing iterations\n",
    "    bnepsilon = 1e-5\n",
    "    \n",
    "    mean, variance = tf.nn.moments(Ylogits, [0])\n",
    "    \n",
    "    update_moving_averages = exp_moving_avg.apply([mean, variance])\n",
    "    m = tf.cond(is_test, lambda: exp_moving_avg.average(mean), lambda: mean)\n",
    "    v = tf.cond(is_test, lambda: exp_moving_avg.average(variance), lambda: variance)\n",
    "    \n",
    "    Ybn = tf.nn.batch_normalization(Ylogits, m, v, None, None, bnepsilon)\n",
    "    Ybn = tf.transpose(Ybn)\n",
    "    return Ybn, update_moving_averages\n",
    "\n",
    "def forward_propagation(X, parameters,tst,iter,pkeep):   \n",
    "    maximo= (len(parameters))\n",
    "    cont =1\n",
    "    l_update_ema=[]\n",
    "    for i in range(1,maximo+1,2):\n",
    "        if (cont==((maximo)/2)):\n",
    "            break\n",
    "        if(i==1):\n",
    "            Z=linear_forward(X,parameters['W'+str(cont)],parameters['b'+str(cont)])\n",
    "            #Z = tf.matmul(parameters['W'+str(cont)],X)\n",
    "            ZB, update_ema = batchnorm(Z, tst,iter)\n",
    "            A = tf.nn.relu(ZB)                                              \n",
    "            #Ad = sess.run(tf.nn.dropout(A, pkeep))\n",
    "            Ad = tf.nn.dropout(A, pkeep)\n",
    "        else:\n",
    "            #Z = tf.matmul(parameters['W'+str(cont)],Ad)\n",
    "            Z=linear_forward(Ad,parameters['W'+str(cont)],parameters['b'+str(cont)])\n",
    "            ZB, update_ema = batchnorm(Z, tst,iter)\n",
    "            A = tf.nn.relu(ZB)                                        \n",
    "            #Ad = tf.nn.dropout(A, pkeep)\n",
    "            Ad = tf.nn.dropout(A, pkeep)\n",
    "        l_update_ema.append(update_ema)\n",
    "        cont=cont+1\n",
    "            \n",
    "    #Z3 = tf.add(tf.matmul(parameters['W'+str(cont)],Ad),parameters['b'+str(cont)])    \n",
    "    Z3=linear_forward(Ad,parameters['W'+str(cont)],parameters['b'+str(cont)])\n",
    "    ##softmax\n",
    "    Y_Y = tf.nn.softmax(tf.transpose(Z3))\n",
    "    if((cont==3)):\n",
    "        update_ema = tf.group(l_update_ema[0],l_update_ema[1])\n",
    "    if((cont==4)):\n",
    "        update_ema = tf.group(l_update_ema[0],l_update_ema[1],l_update_ema[2])\n",
    "    if((cont==5)):\n",
    "        update_ema = tf.group(l_update_ema[0],l_update_ema[1],l_update_ema[2],l_update_ema[3])  \n",
    "    if((cont==6)):\n",
    "        update_ema = tf.group(l_update_ema[0],l_update_ema[1],l_update_ema[2],l_update_ema[3],l_update_ema[4])\n",
    "    if((cont==7)):\n",
    "        update_ema = tf.group(l_update_ema[0],l_update_ema[1],l_update_ema[2],l_update_ema[3],l_update_ema[4],l_update_ema[5])\n",
    "    if((cont==8)):\n",
    "        update_ema = tf.group(l_update_ema[0],l_update_ema[1],l_update_ema[2],l_update_ema[3],l_update_ema[4],l_update_ema[5],l_update_ema[6])\n",
    "    if((cont==9)):\n",
    "        update_ema = tf.group(l_update_ema[0],l_update_ema[1],l_update_ema[2],l_update_ema[3],l_update_ema[4],l_update_ema[5],l_update_ema[6],l_update_ema[7])\n",
    "    if((cont==10)):\n",
    "        update_ema = tf.group(l_update_ema[0],l_update_ema[1],l_update_ema[2],l_update_ema[3],l_update_ema[4],l_update_ema[5],l_update_ema[6],l_update_ema[7],l_update_ema[8])        \n",
    "    return Z3,update_ema,Y_Y\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    Z = tf.add(tf.matmul(W,A),b)  \n",
    "    return Z\n",
    "\n",
    "# GRADED FUNCTION: compute_cost \n",
    "def compute_cost(Z3, Y):\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, \n",
    "          learning_rate = 0.0001,\n",
    "          num_epochs = 30, \n",
    "          minibatch_size = 32, \n",
    "          print_cost = False,\n",
    "          pkeepv=1.0,\n",
    "         layer=[],\n",
    "         inicializa=1):\n",
    "\n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    cost_train = []                                        # To keep track of the cost\n",
    "    cost_test = []\n",
    "    \n",
    "    #print (n_x)\n",
    "    #print (m)\n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    X, Y,tst,iter,pkeep= create_placeholders(n_x, n_y)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    ### inicializar los parametros y la arquitectura de la red\n",
    "    #parameters = initialize_parameters(n_x,n_y,2)\n",
    "    parameters = initialize_parameters(layer,1)\n",
    "    #print (parameters)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    Z3,update_ema,Y_Y = forward_propagation(X, parameters,tst,iter,pkeep)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    cost = compute_cost(Z3,Y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    ##### Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        i=0\n",
    "        for epoch in range(num_epochs):\n",
    "            i=i+1\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "            #print (\"mini \"+str(num_minibatches))\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                #_ , minibatch_cost, _ = sess.run([optimizer, cost, update_ema], feed_dict={X: minibatch_X, Y: minibatch_Y, tst: False, iter: i,pkeep: pkeepv})\n",
    "                ### END CODE HERE ###\n",
    "                minibatch_cost = sess.run(cost, feed_dict={X: minibatch_X, Y: minibatch_Y, tst: False,pkeep:pkeepv})\n",
    "                sess.run(optimizer, feed_dict={X: minibatch_X, Y: minibatch_Y, tst: False, iter: i, pkeep:pkeepv})\n",
    "                sess.run(update_ema, feed_dict={X: minibatch_X, Y: minibatch_Y, tst: False, iter: i, pkeep:pkeepv})\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "                #if(i==30):\n",
    "                #    break\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 10 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            #if print_cost == True and epoch % 5 == 0:\n",
    "            #    cost_train.append(epoch_cost)\n",
    "            #    c_test = sess.run([ cost], feed_dict={X: minibatch_X, Y: minibatch_Y, tst: True,pkeep: 1.0})\n",
    "            #    cost_test.append(c_test)\n",
    "        '''\n",
    "        if(print_cost== True):\n",
    "            # plot the cost\n",
    "            plt.figure(\"Figura1\")\n",
    "            plt.plot(np.squeeze(cost_train))\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('iterations (per tens)')\n",
    "            plt.title(\"Train Learning rate =\" + str(learning_rate))\n",
    "\n",
    "            plt.figure(\"Figura2\")\n",
    "            plt.plot(np.squeeze(cost_test))\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('iterations (per tens)')\n",
    "            plt.title(\"Test Learning rate =\" + str(learning_rate))        \n",
    "\n",
    "\n",
    "            plt.figure(\"Figura3\")\n",
    "            plt.plot(np.squeeze(cost_train))\n",
    "            plt.plot(np.squeeze(cost_test))\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('iterations (per tens)')\n",
    "            plt.title(\"Test Learning rate =\" + str(learning_rate)) \n",
    "            plt.legend(loc=4)\n",
    "            plt.show()      \n",
    "          '''\n",
    "\n",
    "        \n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        #print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions, xq no recibe a3. Claro no lo tengo. ¿como sabe que debe calcularlo?\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "        #z3_value = z3\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        acc_train=sess.run(accuracy, feed_dict ={X: X_train, Y: Y_train,tst: False,pkeep: 1.0})\n",
    "        acc_test, A3_value=sess.run([accuracy,Y_Y] ,feed_dict ={X: X_test, Y: Y_test,tst: True,pkeep: 1.0})\n",
    "        \n",
    "        #print (\"Train Accuracy :\", accuracy.eval({X: X_train, Y: Y_train,tst: False,pkeep: 1.0}))\n",
    "        #print (\"Test Accuracy: \", accuracy.eval({X: X_test, Y: Y_test,tst: True,pkeep: 1.0}))\n",
    "        '''\n",
    "        print (\"Train Accuracy:\", sess.run(accuracy, feed_dict ={X: X_train, Y: Y_train})\n",
    "\n",
    "        print (\"Test Accuracy:\", sess.run(accuracy, feed_dict ={X: X_test, Y: Y_test})\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #print (\"-----Fin prueba------\")\n",
    "        return parameters,acc_train,acc_test,A3_value\n",
    "print (\"Cargo Modelo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzo la carga...\n",
      "antes\n",
      "(70000, 59)\n",
      "despues\n",
      "(70000, 57)\n",
      "Termino cargar\n",
      "(84, 47822)\n",
      "(2, 47822)\n",
      "(84, 8440)\n",
      "(2, 8440)\n",
      "CPU times: user 1min 22s, sys: 532 ms, total: 1min 22s\n",
      "Wall time: 7min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, Y_train, X_test, Y_test = datos()\n",
    "print (X_train.shape)\n",
    "#Y_train=Y_train.reshape(Y_train.shape[0],1)\n",
    "print (Y_train.shape)\n",
    "print (X_test.shape)\n",
    "#Y_test=Y_test.reshape(Y_test.shape[0],1)\n",
    "print (Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquitectura:  84-400-300-200-100-50-25-2  Learning Rate:  0.00011  Dropout  1  Epoch:  1000  Inicializar:  1\n",
      "Cost after epoch 0: 0.516938\n",
      "Cost after epoch 10: 0.275040\n",
      "Cost after epoch 20: 0.244974\n",
      "Cost after epoch 30: 0.223764\n",
      "Cost after epoch 40: 0.206544\n",
      "Cost after epoch 50: 0.193939\n",
      "Cost after epoch 60: 0.185581\n",
      "Cost after epoch 70: 0.175643\n",
      "Cost after epoch 100: 0.155531\n",
      "Cost after epoch 110: 0.150398\n",
      "Cost after epoch 120: 0.148091\n",
      "Cost after epoch 130: 0.143660\n",
      "Cost after epoch 140: 0.138519\n",
      "Cost after epoch 150: 0.134246\n",
      "Cost after epoch 160: 0.131731\n",
      "Cost after epoch 170: 0.127130\n",
      "Cost after epoch 180: 0.125983\n",
      "Cost after epoch 190: 0.123189\n",
      "Cost after epoch 200: 0.120909\n",
      "Cost after epoch 210: 0.117970\n",
      "Cost after epoch 220: 0.114958\n",
      "Cost after epoch 230: 0.113067\n",
      "Cost after epoch 240: 0.112358\n",
      "Cost after epoch 250: 0.110203\n",
      "Cost after epoch 260: 0.107040\n",
      "Cost after epoch 270: 0.106604\n",
      "Cost after epoch 280: 0.104753\n",
      "Cost after epoch 290: 0.101441\n",
      "Cost after epoch 300: 0.100439\n",
      "Cost after epoch 310: 0.101113\n",
      "Cost after epoch 320: 0.098347\n",
      "Cost after epoch 330: 0.097185\n",
      "Cost after epoch 340: 0.096652\n",
      "Cost after epoch 350: 0.094087\n",
      "Cost after epoch 360: 0.092469\n",
      "Cost after epoch 370: 0.092502\n",
      "Cost after epoch 380: 0.091384\n",
      "Cost after epoch 390: 0.088381\n",
      "Cost after epoch 400: 0.088142\n",
      "Cost after epoch 410: 0.087981\n",
      "Cost after epoch 420: 0.084383\n",
      "Cost after epoch 430: 0.087117\n",
      "Cost after epoch 440: 0.085045\n",
      "Cost after epoch 450: 0.082929\n",
      "Cost after epoch 460: 0.081531\n",
      "Cost after epoch 470: 0.081877\n",
      "Cost after epoch 480: 0.083838\n",
      "Cost after epoch 530: 0.078375\n",
      "Cost after epoch 540: 0.074979\n",
      "Cost after epoch 550: 0.076523\n",
      "Cost after epoch 560: 0.074586\n",
      "Cost after epoch 570: 0.075750\n",
      "Cost after epoch 580: 0.074658\n",
      "Cost after epoch 590: 0.073180\n",
      "Cost after epoch 600: 0.070979\n",
      "Cost after epoch 610: 0.073896\n",
      "Cost after epoch 620: 0.072997\n",
      "Cost after epoch 630: 0.071924\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-adfbcffcd20e>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(X_train, Y_train, X_test, Y_test, learning_rate, num_epochs, minibatch_size, print_cost, pkeepv, layer, inicializa)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;31m### END CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mminibatch_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mminibatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mminibatch_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpkeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpkeepv\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                 \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mminibatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mminibatch_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpkeepv\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m                 \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_ema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mminibatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mminibatch_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpkeepv\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(2)\n",
    "#a_learning_rate = [0.0001, 0.009, 0.005, 0.001, 0.09, 0.05, 0.01, 0.9, 0.5, 0.1]\n",
    "#a_pkeepv = [1,0.95,0.90,0.85,0.80]\n",
    "(n_x, m) = X_train.shape                         \n",
    "n_y = Y_train.shape[0] \n",
    "l_layer=[]\n",
    "#l_layer.append([n_x,25,12,n_y])\n",
    "#l_layer.append([n_x,100,50,25,n_y])\n",
    "#l_layer.append([n_x,200,100,50,25,n_y])\n",
    "#l_layer.append([n_x,1000,500,n_y])\n",
    "#l_layer.append([n_x,1000,200,25,n_y])\n",
    "l_layer.append([n_x,400,300,200,100,50,25,n_y])\n",
    "#l_layer.append([n_x,800,400,200,100,50,25,n_y])\n",
    "#l_layer.append([n_x,1600,800,400,200,100,50,25,n_y])\n",
    "### E_learning_rate\n",
    "\n",
    "l_el = np.random.uniform(-4,-2,10)\n",
    "a_learning_rate=np.unique(np.around(np.power(10,l_el),decimals=5))\n",
    "#l_el=-4*np.random.rand(10)\n",
    "\n",
    "#a_learning_rate=np.unique(np.around(np.power(10,l_el),decimals=3))\n",
    "##a_learning_rate = [0.0001]\n",
    "inicializa=1\n",
    "#a_pkeepv = [1,0.85],no generalize al inicio\n",
    "a_pkeepv = [1]\n",
    "nume=1000\n",
    "result=[]\n",
    "for e_layer in l_layer:\n",
    "    for e_learning_rate in a_learning_rate:\n",
    "        for e_pkeepv in a_pkeepv:\n",
    "            print (\"Arquitectura: \",\"-\".join(map(str, e_layer)),\" Learning Rate: \", e_learning_rate, \" Dropout \", e_pkeepv,\" Epoch: \", nume,\" Inicializar: \", inicializa)\n",
    "            parameters,acc_train,acc_test,A3_value = model(X_train, \n",
    "                                                  Y_train, \n",
    "                                                  X_test, \n",
    "                                                  Y_test,\n",
    "                                                  learning_rate=e_learning_rate,\n",
    "                                                  pkeepv=e_pkeepv,\n",
    "                                                  num_epochs=nume,\n",
    "                                                  print_cost=True,\n",
    "                                                  layer=e_layer,\n",
    "                                                inicializa=2)\n",
    "            result.append([e_layer,e_learning_rate,e_pkeepv,acc_train,acc_test,nume,inicializa])\n",
    "            print (\"Accuracity Train: \", acc_train,\"Accuracity Test: \" ,acc_test)\n",
    "            print(\"----- ----- -----\")\n",
    "\n",
    "fecha=time.strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "aresu=np.asarray(result)\n",
    "aresu=aresu[np.argsort(aresu[:, 4])].tolist()\n",
    "resultado = '\\n'.join(map(str, aresu))\n",
    "f = open ('resultado/archivo'+fecha+'.txt',\"w\")\n",
    "f.write(resultado)\n",
    "f.close()\n",
    "print(\"----Termino-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02828412  0.97171581]\n",
      " [ 0.04219292  0.95780712]\n",
      " [ 0.04681063  0.95318937]\n",
      " [ 0.05038058  0.94961935]\n",
      " [ 0.39675713  0.60324293]\n",
      " [ 0.1171367   0.88286334]\n",
      " [ 0.09787513  0.90212482]\n",
      " [ 0.06176955  0.93823045]\n",
      " [ 0.15814407  0.84185594]\n",
      " [ 0.06751215  0.93248779]]\n"
     ]
    }
   ],
   "source": [
    "print (A3_value[0:10,0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultado = '\\n'.join(map(str, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A3_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[list([84, 25, 12, 2]), 0.021473952157175989, 0.85, 0.87800592,\n",
       "        0.88364929],\n",
       "       [list([84, 25, 12, 2]), 0.001314322752836385, 0.85, 0.88593113,\n",
       "        0.88720381],\n",
       "       [list([84, 25, 12, 2]), 0.99894712366592076, 0.85, 0.87800592,\n",
       "        0.88364929],\n",
       "       [list([84, 25, 12, 2]), 0.061754656372527347, 0.85, 0.87800592,\n",
       "        0.88364929],\n",
       "       [list([84, 25, 12, 2]), 0.25880724917679454, 0.85, 0.87800592,\n",
       "        0.88364929],\n",
       "       [list([84, 25, 12, 2]), 0.42721414228233884, 0.85, 0.87800592,\n",
       "        0.88364929],\n",
       "       [list([84, 25, 12, 2]), 0.17987017387999535, 0.85, 0.87800592,\n",
       "        0.88364929],\n",
       "       [list([84, 25, 12, 2]), 0.041472201678756003, 0.85, 0.87800592,\n",
       "        0.88364929],\n",
       "       [list([84, 25, 12, 2]), 0.025877963923366044, 0.85, 0.87800592,\n",
       "        0.88364929],\n",
       "       [list([84, 25, 12, 2]), 0.0069941197526672593, 0.85, 0.88197899,\n",
       "        0.88412321],\n",
       "       [list([84, 100, 50, 25, 2]), 0.021473952157175989, 0.85, 0.88513654,\n",
       "        0.88886255],\n",
       "       [list([84, 100, 50, 25, 2]), 0.001314322752836385, 0.85, 0.88900506,\n",
       "        0.89004737],\n",
       "       [list([84, 100, 50, 25, 2]), 0.99894712366592076, 0.85, 0.87800592,\n",
       "        0.88364929],\n",
       "       [list([84, 100, 50, 25, 2]), 0.061754656372527347, 0.85, 0.87819415,\n",
       "        0.88364929],\n",
       "       [list([84, 100, 50, 25, 2]), 0.25880724917679454, 0.85, 0.87800592,\n",
       "        0.88364929],\n",
       "       [list([84, 100, 50, 25, 2]), 0.42721414228233884, 0.85, 0.87800592,\n",
       "        0.88364929],\n",
       "       [list([84, 100, 50, 25, 2]), 0.17987017387999535, 0.85, 0.87800592,\n",
       "        0.88364929],\n",
       "       [list([84, 100, 50, 25, 2]), 0.041472201678756003, 0.85, 0.88055706,\n",
       "        0.88436019],\n",
       "       [list([84, 100, 50, 25, 2]), 0.025877963923366044, 0.85, 0.88676763,\n",
       "        0.88981044],\n",
       "       [list([84, 100, 50, 25, 2]), 0.0069941197526672593, 0.85,\n",
       "        0.88718581, 0.88827014]], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aresu=np.asarray(result)\n",
    "aresu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[list([84, 25, 12, 2]), 0.0069941197526672593, 0.85, 0.88197899,\n",
    "        0.88412321],\n",
    "   [list([84, 25, 12, 2]), 0.0069941197526672593, 0.85, 0.88197899,\n",
    "        0.88412321]\n",
    "   [list([84, 100, 50, 25, 2]), 0.0069941197526672593, 0.85,\n",
    "        0.88718581, 0.88827014]], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aresu=np.asarray(result)\n",
    "aresu=aresu[np.argsort(aresu[:, 4])].tolist()\n",
    "resultado = '\\n'.join(map(str, aresu))\n",
    "f = open ('resultado/archivoprueba'+fecha+'.txt',\"w\")\n",
    "f.write(resultado)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[84, 25, 12, 2], 0.021473952157175989, 0.85, 0.87800592, 0.88364929],\n",
       " [[84, 100, 50, 25, 2], 0.17987017387999535, 0.85, 0.87800592, 0.88364929],\n",
       " [[84, 25, 12, 2], 0.99894712366592076, 0.85, 0.87800592, 0.88364929],\n",
       " [[84, 25, 12, 2], 0.061754656372527347, 0.85, 0.87800592, 0.88364929],\n",
       " [[84, 25, 12, 2], 0.25880724917679454, 0.85, 0.87800592, 0.88364929],\n",
       " [[84, 25, 12, 2], 0.42721414228233884, 0.85, 0.87800592, 0.88364929],\n",
       " [[84, 25, 12, 2], 0.17987017387999535, 0.85, 0.87800592, 0.88364929],\n",
       " [[84, 25, 12, 2], 0.041472201678756003, 0.85, 0.87800592, 0.88364929],\n",
       " [[84, 25, 12, 2], 0.025877963923366044, 0.85, 0.87800592, 0.88364929],\n",
       " [[84, 100, 50, 25, 2], 0.42721414228233884, 0.85, 0.87800592, 0.88364929],\n",
       " [[84, 100, 50, 25, 2], 0.25880724917679454, 0.85, 0.87800592, 0.88364929],\n",
       " [[84, 100, 50, 25, 2], 0.061754656372527347, 0.85, 0.87819415, 0.88364929],\n",
       " [[84, 100, 50, 25, 2], 0.99894712366592076, 0.85, 0.87800592, 0.88364929],\n",
       " [[84, 25, 12, 2], 0.0069941197526672593, 0.85, 0.88197899, 0.88412321],\n",
       " [[84, 100, 50, 25, 2], 0.041472201678756003, 0.85, 0.88055706, 0.88436019],\n",
       " [[84, 25, 12, 2], 0.001314322752836385, 0.85, 0.88593113, 0.88720381],\n",
       " [[84, 100, 50, 25, 2], 0.0069941197526672593, 0.85, 0.88718581, 0.88827014],\n",
       " [[84, 100, 50, 25, 2], 0.021473952157175989, 0.85, 0.88513654, 0.88886255],\n",
       " [[84, 100, 50, 25, 2], 0.025877963923366044, 0.85, 0.88676763, 0.88981044],\n",
       " [[84, 100, 50, 25, 2], 0.001314322752836385, 0.85, 0.88900506, 0.89004737]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
