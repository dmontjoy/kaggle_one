{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cargo\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "np.random.seed(1)\n",
    "print (\"cargo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def datos():\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    mnist = input_data.read_data_sets('/tmp/tensorflow/mnist/input_data', one_hot=True)\n",
    "    print (\"Dimensiones train\")\n",
    "    print (mnist.train.images.shape)\n",
    "    print (\"Dimensiones test\")\n",
    "    print (mnist.test.images.shape)\n",
    "    print (\"cargo\")\n",
    "    X_train_o = mnist.train.images[:55000,:]\n",
    "    Y_train_o = mnist.train.labels[:55000,:]\n",
    "    X_test_o = mnist.test.images[:10000,:]\n",
    "    Y_test_o = mnist.test.labels[:10000,:]\n",
    "    print (\"Dimensiones X train\")\n",
    "    print (X_train_o.shape)\n",
    "    print (\"Dimensiones Y train\")\n",
    "    print (Y_train_o.shape)\n",
    "    print (\"Las dimensiones de X son Nx por M, donde Nx son las caracteristicas y M es el numero, por eso hay que trasponerlo\")\n",
    "    X_train=X_train_o.T\n",
    "    Y_train=Y_train_o.T\n",
    "    print (\"........\")\n",
    "    print (\"X train transpuesto\", str(X_train.shape))\n",
    "    print (\"Y train transpuesto\", str(Y_train.shape))\n",
    "    X_test=X_test_o.T\n",
    "    Y_test=Y_test_o.T\n",
    "    print (\"X test transpuesto\", str(X_test.shape))\n",
    "    print (\"Y test transpuesto\", str(Y_test.shape))\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    X = tf.placeholder(tf.float32,shape=(n_x, None),name=\"X\")\n",
    "    Y = tf.placeholder(tf.float32,shape=(n_y, None),name=\"Y\")\n",
    "    ### Otras variables ###\n",
    "    \n",
    "     # variable learning rate\n",
    "    lr = tf.placeholder(tf.float32)\n",
    "    # train/test selector for batch normalisation\n",
    "    tst = tf.placeholder(tf.bool)\n",
    "    ##Drop Out\n",
    "    pkeep = tf.placeholder(tf.float32)\n",
    "    # training iteration\n",
    "    iter = tf.placeholder(tf.int32)   \n",
    "    #Ad = tf.placeholder(tf.float32,shape=(None, None),name=\"X\")\n",
    "    \n",
    "    return X, Y, tst,pkeep, iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(layer,tipo=1):\n",
    "    parameters={}\n",
    "    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n",
    "    L=len(layer)\n",
    "    for i in range(1, L):\n",
    "        if(tipo==1):\n",
    "            parameters['W' + str(i)] =tf.get_variable('W' + str(i), [layer[i], layer[i-1]], initializer = tf.contrib.layers.xavier_initializer(seed = 1))        \n",
    "            parameters['b' + str(i)] =tf.get_variable('b' + str(i), [layer[i], 1], initializer = tf.zeros_initializer()) \n",
    "        if(tipo==2):\n",
    "            parameters['W' + str(i)] =tf.Variable(np.random.randn(layer[i], layer[i-1])*0.01,dtype=tf.float32,name='W' + str(i))        \n",
    "            parameters['b' + str(i)] =tf.get_variable('b' + str(i), [layer[i], 1], initializer = tf.zeros_initializer())\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchnorm(Ylogits, is_test,iter):\n",
    "    Ylogits = tf.transpose(Ylogits)\n",
    "    exp_moving_avg = tf.train.ExponentialMovingAverage(0.999,iter) # adding the iteration prevents from averaging across non-existing iterations\n",
    "    bnepsilon = 1e-5\n",
    "    ##con 0 el calculo es vertical\n",
    "    mean, variance = tf.nn.moments(Ylogits, [0])\n",
    "    \n",
    "    update_moving_averages = exp_moving_avg.apply([mean, variance])\n",
    "    m = tf.cond(is_test, lambda: exp_moving_avg.average(mean), lambda: mean)\n",
    "    v = tf.cond(is_test, lambda: exp_moving_avg.average(variance), lambda: variance)\n",
    "    \n",
    "    Ybn = tf.nn.batch_normalization(Ylogits, m, v, None, None, bnepsilon)\n",
    "    Ybn = tf.transpose(Ybn)\n",
    "    return Ybn, update_moving_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters,tst,iter,pkeep):   \n",
    "    maximo= (len(parameters))\n",
    "    cont =1\n",
    "    l_update_ema=[]\n",
    "    for i in range(1,maximo+1,2):\n",
    "        if (cont==((maximo)/2)):\n",
    "            break\n",
    "        if(i==1):\n",
    "            Z=linear_forward(X,parameters['W'+str(cont)],parameters['b'+str(cont)])\n",
    "            #Z = tf.matmul(parameters['W'+str(cont)],X)\n",
    "            ZB, update_ema = batchnorm(Z, tst,iter)\n",
    "            A = tf.nn.relu(ZB)                                              \n",
    "            #Ad = sess.run(tf.nn.dropout(A, pkeep))\n",
    "            Ad = tf.nn.dropout(A, pkeep)\n",
    "        else:\n",
    "            #Z = tf.matmul(parameters['W'+str(cont)],Ad)\n",
    "            Z=linear_forward(Ad,parameters['W'+str(cont)],parameters['b'+str(cont)])\n",
    "            ZB, update_ema = batchnorm(Z, tst,iter)\n",
    "            A = tf.nn.relu(ZB)                                        \n",
    "            #Ad = tf.nn.dropout(A, pkeep)\n",
    "            Ad = tf.nn.dropout(A, pkeep)\n",
    "        l_update_ema.append(update_ema)\n",
    "        cont=cont+1\n",
    "            \n",
    "    #Z3 = tf.add(tf.matmul(parameters['W'+str(cont)],Ad),parameters['b'+str(cont)])    \n",
    "    Z3=linear_forward(Ad,parameters['W'+str(cont)],parameters['b'+str(cont)])\n",
    "    if((cont==3)):\n",
    "        update_ema = tf.group(l_update_ema[0],l_update_ema[1])\n",
    "    if((cont==4)):\n",
    "        update_ema = tf.group(l_update_ema[0],l_update_ema[1],l_update_ema[2])\n",
    "    if((cont==5)):\n",
    "        update_ema = tf.group(l_update_ema[0],l_update_ema[1],l_update_ema[2],update_ema[3])  \n",
    "    if((cont==6)):\n",
    "        update_ema = tf.group(l_update_ema[0],l_update_ema[1],l_update_ema[2],update_ema[3],update_ema[4])      \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = tf.add(tf.matmul(W,A),b)  \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost \n",
    "def compute_cost(Z3, Y):\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 2, minibatch_size = 32, print_cost = True):\n",
    "\n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    layer=[n_x,25,12,n_y]\n",
    "    #print (n_x)\n",
    "    #print (m)\n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    X, Y,tst, pkeep,iter= create_placeholders(n_x, n_y)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    ### inicializar los parametros y la arquitectura de la red\n",
    "    #parameters = initialize_parameters(n_x,n_y)\n",
    "    parameters = initialize_parameters(layer,tipo=1)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    Z3 = forward_propagation(X, parameters,tst,iter,pkeep)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    cost = compute_cost(Z3,Y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    ##### Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        i=0\n",
    "        for epoch in range(num_epochs):\n",
    "            i=i+1\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "            #print (\"mini \"+str(num_minibatches))\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                #_ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y, tst: False,pkeep:0.75,iter: epoch})\n",
    "                ### END CODE HERE ###\n",
    "                minibatch_cost = sess.run(cost, feed_dict={X: minibatch_X, Y: minibatch_Y, tst: False,pkeep:0.75,iter: epoch})\n",
    "                sess.run(optimizer, feed_dict={X: minibatch_X, Y: minibatch_Y, tst: False, pkeep:0.75, iter: epoch})\n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "                #if(i==30):\n",
    "                #    break\n",
    "            \n",
    "            #_ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y, tst: False,pkeep:0.75})\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "                print (\"epoch\",str(epoch))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "  \n",
    "\n",
    "        # plot the cost\n",
    "        \n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions, xq no recibe a3. Claro no lo tengo. ¿como sabe que debe calcularlo?\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train,tst: False,pkeep:0.75}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test,tst: True,pkeep:1}))\n",
    "        '''\n",
    "        print (\"Train Accuracy:\", sess.run(accuracy, feed_dict ={X: X_train, Y: Y_train})\n",
    "\n",
    "        print (\"Test Accuracy:\", sess.run(accuracy, feed_dict ={X: X_test, Y: Y_test})\n",
    "        \n",
    "        '''\n",
    "        print (\"Termino\")\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n",
      "Dimensiones train\n",
      "(55000, 784)\n",
      "Dimensiones test\n",
      "(10000, 784)\n",
      "cargo\n",
      "Dimensiones X train\n",
      "(55000, 784)\n",
      "Dimensiones Y train\n",
      "(55000, 10)\n",
      "Las dimensiones de X son Nx por M, donde Nx son las caracteristicas y M es el numero, por eso hay que trasponerlo\n",
      "........\n",
      "X train transpuesto (784, 55000)\n",
      "Y train transpuesto (10, 55000)\n",
      "X test transpuesto (784, 10000)\n",
      "Y test transpuesto (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = datos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 1.823505\n",
      "epoch 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAHiRJREFUeJzt3XuYlXW99/H3R0CNQAQZCY+Y2zxU\nRjVhPbWLsjy0s7SsZGcS1UO2dT/VbpdaXmGZz2Wa22qzC7Fw9NlKloftoYOal0pZWoMOAhaBeCJI\nBtHUPBT4ff64f6O3y7VmFvzmnjUDn9d13dda8zut34/R9Zn7sO6liMDMzGxzbdPqCZiZ2dDmIDEz\nsywOEjMzy+IgMTOzLA4SMzPL4iAxM7MsDhLbakn6maTprZ6H2VDnILEBJ+k+Se9s9Twi4vCIuLDV\n8wCQdLOkTw7A62wnaZ6kxyT9WdK/9dH+c6ndX1K/7Up1kyTdJOlJSX+o/Z320fd0SYslbZB0Wr8v\n1AaUg8S2SJKGt3oOPQbTXIDTgH2APYG3A1+UdFi9hpIOBU4GDgYmAS8HvlpqMh+4E9gJ+DJwmaS2\nJvuuAL4I/KRfVmUt5SCxQUXSeyR1SXpU0q8lHViqO1nSPZIel3S3pKNKdR+TdKukcyWtB05LZb+S\n9E1Jj0i6V9LhpT7P7QU00XYvSQvSa/9C0n9J+u8Ga5gqaZWkkyT9GbhA0lhJ10rqTuNfK2m31P4M\n4B+B2ZKekDQ7le8n6QZJ6yUtk/ShfvgnPg44PSIeiYjfA+cDH2vQdjrwg4hYGhGPAKf3tJX0CuB1\nwKyIeCoiLgcWAx/oqy9ARFwYET8DHu+HNVmLOUhs0JD0OmAe8CmKv3LPA64uHRK5h+INdwzFX7f/\nLWliaYiDgJXAzsAZpbJlwHjgLOAHktRgCr21vQT4bZrXacBH+1jOy4BxFH/5z6T4f+2C9PMewFPA\nbICI+DLwS+DEiBgVESdKeilwQ3rdnYFpwHclvbLei0n6bgrfettdqc1YYBdgUanrIqDumKm8tu0E\nSTulupUR8XhN/Sub6GtbGAeJDSb/GzgvIm6PiI3p/MUzwBsBIuLHEbE6Ip6NiEuB5cCUUv/VEfGf\nEbEhIp5KZfdHxPkRsRG4EJgITGjw+nXbStoDeAPwlYj4W0T8Cri6j7U8S/HX+jPpL/aHI+LyiHgy\nvfmeAbytl/7vAe6LiAvSeu4ALgeOrtc4Iv4lInZssPXs1Y1Kj38pdf0LMLrBHEbVaUtqX1tXO1Zv\nfW0L4yCxwWRP4PPlv6aB3Sn+ikbScaXDXo8Cr6LYe+jxYJ0x/9zzJCKeTE9H1WnXW9tdgPWlskav\nVdYdEU/3/CBppKTzJN0v6TFgAbCjpGEN+u8JHFTzb/ERij2dzfVEetyhVLYDjQ8vPVGnLal9bV3t\nWL31tS2Mg8QGkweBM2r+mh4ZEfMl7UlxPP9EYKeI2BFYApQPU1V1K+s1wDhJI0tlu/fRp3Yunwf2\nBQ6KiB2At6ZyNWj/IHBLzb/FqIj4dL0XkzQnnV+pty0FSOcq1gCvKXV9DbC0wRqW1mn7UEQ8nOpe\nLml0Tf3SJvraFsZBYq0yQtL2pW04RVAcL+kgFV4q6Z/Sm9VLKd5suwEkzaDYI6lcRNwPdFKcwN9W\n0puAIzZxmNEU50UelTQOmFVT/xDFlU09rgVeIemjkkak7Q2S9m8wx+NT0NTbyudALgJOTSf/96M4\nnNjRYM4XAZ+QdEA6v3JqT9uI+CPQBcxKv7+jgAMpDr/12hcgrWd7iveg4WmMRntnNsg5SKxVfkrx\nxtqznRYRnRRvbLOBRyguEf0YQETcDZwD/IbiTffVwK0DON+PAG8CHga+DlxKcf6mWd8CXgKsA24D\nfl5T/23g6HRF13fSeZRDgGOA1RSH3b4BbEeeWRQXLdwP3AKcHRE/B5C0R9qD2QMglZ8F3JTa388L\nA/AYoJ3id3UmcHREdDfZ93yK3/s0ikuHn6LvCxhskJK/2Mps00m6FPhDRNTuWZhtdbxHYtaEdFhp\nb0nbqPgA3/uA/2n1vMwGg8H0iVuzwexlwBUUnyNZBXw6Iu5s7ZTMBgcf2jIzsyw+tGVmZlm2ikNb\n48ePj0mTJrV6GmZmQ8rChQvXRURbX+22iiCZNGkSnZ2drZ6GmdmQIun+Ztr50JaZmWVxkJiZWRYH\niZmZZXGQmJlZFgeJmZllcZCYmVkWB4mZmWVxkJiZWRYHiZmZZXGQmJlZFgeJmZllcZCYmVkWB4mZ\nmWVxkJiZWRYHiZmZZXGQmJlZFgeJmZllcZCYmVkWB4mZmWVxkJiZWRYHiZmZZak0SCTNk7RW0pIG\n9WMlXSnpLkm/lfSqUt1hkpZJWiHp5FL5XpJul7Rc0qWStq1yDWZm1ruq90g6gMN6qf8S0BURBwLH\nAd8GkDQM+C/gcOAAYJqkA1KfbwDnRsQ+wCPAJ6qZupmZNaPSIImIBcD6XpocANyY2v4BmCRpAjAF\nWBERKyPib8APgfdJEvAO4LLU/0LgyKrmb2ZmfWv1OZJFwPsBJE0B9gR2A3YFHiy1W5XKdgIejYgN\nNeUvImmmpE5Jnd3d3RVN38zMWh0kZwJjJXUB/wrcCWwAVKdt9FL+4sKIuRHRHhHtbW1t/TVfMzOr\nMbyVLx4RjwEzANJhq3vTNhLYvdR0N2A1sA7YUdLwtFfSU25mZi3S0j0SSTuWrrr6JLAghcvvgH3S\nFVrbAscAV0dEADcBR6c+04GrBnreZmb2vEr3SCTNB6YC4yWtAmYBIwAiYg6wP3CRpI3A3aQrsCJi\ng6QTgeuAYcC8iFiahj0J+KGkr1McCvtBlWswM7Peqfgjf8vW3t4enZ2drZ6GmdmQImlhRLT31a7V\nJ9vNzGyIc5CYmVkWB4mZmWVxkJiZWRYHiZmZZXGQmJlZFgeJmZllcZCYmVkWB4mZmWVxkJiZWRYH\niZmZZXGQmJlZFgeJmZllcZCYmVkWB4mZmWVxkJiZWRYHiZmZZXGQmJlZlsqCRNI8SWslLWlQP0bS\nNZIWSVoqaUYqf7ukrtL2tKQjU12HpHtLdZOrmr+ZmTVneIVjdwCzgYsa1J8A3B0RR0hqA5ZJujgi\nbgImA0gaB6wAri/1+0JEXFbdtM3MbFNUtkcSEQuA9b01AUZLEjAqtd1Q0+Zo4GcR8WQ1szQzs1yt\nPEcyG9gfWA0sBj4TEc/WtDkGmF9TdoakuySdK2m7RoNLmimpU1Jnd3d3v07czMye18ogORToAnah\nOJQ1W9IOPZWSJgKvBq4r9TkF2A94AzAOOKnR4BExNyLaI6K9ra2tgumbmRm0NkhmAFdEYQVwL0VI\n9PgQcGVE/L2nICLWpPbPABcAUwZ0xmZm9iKtDJIHgIMBJE0A9gVWluqnUXNYK+2lkM6rHAnUvSLM\nzMwGTmVXbUmaD0wFxktaBcwCRgBExBzgdKBD0mJAwEkRsS71nQTsDtxSM+zF6QovURwWO76q+ZuZ\nWXMqC5KImNZH/WrgkAZ19wG71il/R79MzszM+o0/2W5mZlkcJGZmlsVBYmZmWRwkZmaWxUFiZmZZ\nHCRmZpbFQWJmZlkcJGZmlsVBYmZmWRwkZmaWxUFiZmZZHCRmZpbFQWJmZlkcJGZmlsVBYmZmWRwk\nZmaWxUFiZmZZHCRmZpal0iCRNE/SWklLGtSPkXSNpEWSlkqaUarbKKkrbVeXyveSdLuk5ZIulbRt\nlWswM7PeVb1H0gEc1kv9CcDdEfEaYCpwTikYnoqIyWl7b6nPN4BzI2If4BHgE/0/bTMza1alQRIR\nC4D1vTUBRksSMCq13dCocWr3DuCyVHQhcGT/zNbMzDZHq8+RzAb2B1YDi4HPRMSzqW57SZ2SbpPU\nExY7AY9GRE/YrAJ2rTewpJmpf2d3d3eFSzAz27q1OkgOBbqAXYDJwGxJO6S6PSKiHfhn4FuS9gZU\nZ4yoN3BEzI2I9ohob2trq2DqZmYGrQ+SGcAVUVgB3AvsBxARq9PjSuBm4LXAOmBHScNT/90o9mbM\nzKxFWh0kDwAHA0iaAOwLrJQ0VtJ2qXw88GaKk/IB3AQcnfpPB64a8FmbmdlzhvfdZPNJmk9xNdZ4\nSauAWcAIgIiYA5wOdEhaTHHY6qSIWCfpfwHnSXqWIuzOjIi707AnAT+U9HXgTuAHVa7BzMx6V2mQ\nRMS0PupXA4fUKf818OoGfVYCU/plgmZmlq3Vh7bMzGyIc5CYmVkWB4mZmWVxkJiZWRYHiZmZZXGQ\nmJlZFgeJmZllcZCYmVkWB4mZmWVxkJiZWRYHiZmZZXGQmJlZFgeJmZllcZCYmVkWB4mZmWVpKkgk\nfbCZMjMz2/o0u0dySpNlZma2len1GxIlHQ68G9hV0ndKVTsAG6qcmJmZDQ197ZGsBjqBp4GFpe1q\n4NDeOkqaJ2mtpCUN6sdIukbSIklLJc1I5ZMl/SaV3SXpw6U+HZLuldSVtsnNL9XMzKrQ6x5JRCwC\nFkm6JCL+DiBpLLB7RDzSx9gdwGzgogb1JwB3R8QRktqAZZIuBp4EjouI5ZJ2ARZKui4iHk39vhAR\nlzW1OjMzq1yz50hukLSDpHHAIuACSf/RW4eIWACs760JMFqSgFGp7YaI+GNELE9jrAbWAm1NztPM\nzAZYs0EyJiIeA94PXBARrwfemfnas4H9KQ6fLQY+ExHPlhtImgJsC9xTKj4jHfI6V9J2jQaXNFNS\np6TO7u7uzKmamVkjzQbJcEkTgQ8B1/bTax8KdAG7AJOB2ZJ26KlMr/f/gBmlgDkF2A94AzAOOKnR\n4BExNyLaI6K9rc07NGZmVWk2SL4GXAfcExG/k/RyYHnma88ArojCCuBeipAgBcpPgFMj4raeDhGx\nJrV/BrgAmJI5BzMzy9RUkETEjyPiwIj4dPp5ZUR8IPO1HwAOBpA0AdgXWClpW+BK4KKI+HG5Q9pL\nIZ1XORKoe0WYmZkNnF6v2uohaTfgP4E3U5wk/xXFOY1VvfSZD0wFxktaBcwCRgBExBzgdKBD0mJA\nwEkRsU7SscBbgZ0kfSwN97GI6AIuTld4ieKw2PGbtlwzM+tvioi+G0k3AJdQnLMAOBb4SES8q8K5\n9Zv29vbo7Oxs9TTMzIYUSQsjor2vds2eI2mLiAsiYkPaOvAluWZmRvNBsk7SsZKGpe1Y4OEqJ2Zm\nZkNDs0HycYpLf/8MrAGOprjqyszMtnJNnWynODE+vee2KOkT7t+kCBgzM9uKNbtHcmD53loRsR54\nbTVTMjOzoaTZINkm3awReG6PpNm9GTMz24I1GwbnAL+WdBnF50g+BJxR2azMzGzIaCpIIuIiSZ3A\nOyg+DPj+iLi70pmZmdmQ0PThqRQcDg8zM3uBZs+RmJmZ1eUgMTOzLA4SMzPL4iAxM7MsDhIzM8vi\nIDEzsywOEjMzy+IgMTOzLA4SMzPLUmmQSJonaa2kJQ3qx0i6RtIiSUslzSjVTZe0PG3TS+Wvl7RY\n0gpJ35GkKtdgZma9q3qPpAM4rJf6E4C7I+I1wFTgHEnbprsLzwIOAqYAs0p3H/4eMBPYJ229jW9m\nZhWrNEgiYgGwvrcmwOi0VzEqtd0AHArcEBHr0/eg3AAcJmkisENE/CYiArgIOLLKNZiZWe9a/Z0i\ns4GrgdXAaODDEfGspF2BB0vtVgG7pm1VnXIzM2uRVp9sPxToAnYBJgOzJe1Acav6WtFL+YtImimp\nU1Jnd3d3f83XzMxqtDpIZgBXRGEFcC+wH8Wexu6ldrtR7LWsSs9ry18kIuZGRHtEtLe1tVUyeTMz\na32QPAAcDCBpArAvsBK4DjhE0th0kv0Q4LqIWAM8LumN6bzKccBVrZm6mZlBxedIJM2nuBprvKRV\nFFdijQCIiDnA6UCHpMUUh61Oioh1qe/pwO/SUF+LiJ6T9p+muBrsJcDP0mZmZi2i4uKnLVt7e3t0\ndna2ehpmZkOKpIUR0d5Xu1Yf2jIzsyHOQWJmZlkcJGZmlsVBYmZmWRwkZmaWxUFiZmZZHCRmZpbF\nQWJmZlkcJGZmlsVBYmZmWRwkZmaWxUFiZmZZHCRmZpbFQWJmZlkcJGZmlsVBYmZmWRwkZmaWxUFi\nZmZZHCRmZpalsiCRNE/SWklLGtR/QVJX2pZI2ihpnKR9S+Vdkh6T9NnU5zRJfyrVvbuq+ZuZWXOG\nVzh2BzAbuKheZUScDZwNIOkI4HMRsR5YD0xO5cOAPwFXlrqeGxHfrG7aZma2KSrbI4mIBRSh0Ixp\nwPw65QcD90TE/f02MTMz61ctP0ciaSRwGHB5nepjeHHAnCjprnTobGwv486U1Cmps7u7ux9nbGZm\nZS0PEuAI4NZ0WOs5krYF3gv8uFT8PWBvikNfa4BzGg0aEXMjoj0i2tva2vp/1mZmBgyOIKm31wFw\nOHBHRDzUUxARD0XExoh4FjgfmDJAczQzswZaGiSSxgBvA66qU/2i8yaSJpZ+PAqoe0WYmZkNnMqu\n2pI0H5gKjJe0CpgFjACIiDmp2VHA9RHx15q+I4F3AZ+qGfYsSZOBAO6rU29mZgOssiCJiGlNtOmg\nuEy4tvxJYKc65R/tj7mZmVn/GQznSMzMbAhzkJiZWRYHiZmZZXGQmJlZFgeJmZllcZCYmVkWB4mZ\nmWVxkJiZWRYHiZmZZXGQmJlZFgeJmZllcZCYmVkWB4mZmWVxkJiZWRYHiZmZZXGQmJlZFgeJmZll\ncZCYmVmWyoJE0jxJayUtaVD/BUldaVsiaaOkcanuPkmLU11nqc84STdIWp4ex1Y1fzMza06VeyQd\nwGGNKiPi7IiYHBGTgVOAWyJifanJ21N9e6nsZODGiNgHuDH9bGZmLVRZkETEAmB9nw0L04D5TbR7\nH3Bhen4hcORmTM3MzPpRy8+RSBpJsedyeak4gOslLZQ0s1Q+ISLWAKTHnXsZd6akTkmd3d3dVUzd\nzMwYBEECHAHcWnNY680R8TrgcOAESW/d1EEjYm5EtEdEe1tbW3/N1czMagyGIDmGmsNaEbE6Pa4F\nrgSmpKqHJE0ESI9rB3CeZmZWR0uDRNIY4G3AVaWyl0oa3fMcOAToufLramB6ej693M/MzFpjeFUD\nS5oPTAXGS1oFzAJGAETEnNTsKOD6iPhrqesE4EpJPfO7JCJ+nurOBH4k6RPAA8AHq5q/mZk1RxHR\n6jlUrr29PTo7O/tuaGZmz5G0sOYjGHUNhnMkZmY2hDlIzMwsi4PEzMyyOEjMzCyLg8TMzLI4SMzM\nLIuDxMzMsjhIzMwsi4PEzMyyOEjMzCyLg8TMzLI4SMzMLIuDxMzMsjhIzMwsi4PEzMyyOEjMzCyL\ng8TMzLI4SMzMLEtlQSJpnqS1kpY0qP+CpK60LZG0UdI4SbtLuknS7yUtlfSZUp/TJP2p1O/dVc3f\nzMyaU+UeSQdwWKPKiDg7IiZHxGTgFOCWiFgPbAA+HxH7A28ETpB0QKnruT39IuKnFc7fzMyaUFmQ\nRMQCYH2TzacB81O/NRFxR3r+OPB7YNdKJmlmZtlafo5E0kiKPZfL69RNAl4L3F4qPlHSXenQ2dhe\nxp0pqVNSZ3d3dz/P2szMerQ8SIAjgFvTYa3nSBpFES6fjYjHUvH3gL2BycAa4JxGg0bE3Ihoj4j2\ntra2amZuZmYMb/UEgGNIh7V6SBpBESIXR8QVPeUR8VCpzfnAtc28wMKFC9dJur9/pjugxgPrWj2J\nAbS1rRe85q3FUF3zns00ammQSBoDvA04tlQm4AfA7yPiP2raT4yINenHo4C6V4TVioghuUsiqTMi\n2ls9j4Gyta0XvOatxZa+5sqCRNJ8YCowXtIqYBYwAiAi5qRmRwHXR8RfS13fDHwUWCypK5V9KV2h\ndZakyUAA9wGfqmr+ZmbWnMqCJCKmNdGmg+Iy4XLZrwA1aP/R/pibmZn1n8Fwst0am9vqCQywrW29\n4DVvLbboNSsiWj0HMzMbwrxHYmZmWRwkZmaWxUHSYulGlTdIWp4e635aX9L01Ga5pOl16q9udIPM\nwSRnvZJGSvqJpD+kG3qeObCz3zSSDpO0TNIKSSfXqd9O0qWp/vZ0J4eeulNS+TJJhw7kvHNs7pol\nvUvSQkmL0+M7Bnrumyvn95zq95D0hKR/H6g597uI8NbCDTgLODk9Pxn4Rp0244CV6XFsej62VP9+\n4BJgSavXU+V6gZHA21ObbYFfAoe3ek0N1jkMuAd4eZrrIuCAmjb/AsxJz48BLk3PD0jttwP2SuMM\na/WaKl7za4Fd0vNXAX9q9XqqXnOp/nLgx8C/t3o9m7t5j6T13gdcmJ5fCBxZp82hwA0RsT4iHgFu\nIN1ZOd1K5t+Arw/AXPvDZq83Ip6MiJsAIuJvwB3AbgMw580xBVgRESvTXH9Isfay8r/FZcDB6QO5\n7wN+GBHPRMS9wIo03mC32WuOiDsjYnUqXwpsL2m7AZl1npzfM5KOpPhDaekAzbcSDpLWmxDp0/rp\ncec6bXYFHiz9vIrn74h8OsU9x56scpL9KHe9AEjakeI+bTdWNM9cfa6h3CYiNgB/AXZqsu9glLPm\nsg8Ad0bEMxXNsz9t9polvRQ4CfjqAMyzUoPhXltbPEm/AF5Wp+rLzQ5RpyzSp/z/ISI+V3vctZWq\nWm9p/OEU92f7TkSs3PQZDohe19BHm2b6DkY5ay4qpVcC3wAO6cd5VSlnzV+l+H6lJ9IOypDlIBkA\nEfHORnWSHuq5h5ikicDaOs1WUdxupsduwM3Am4DXS7qP4ne5s6SbI2IqLVThenvMBZZHxLf6YbpV\nWQXsXvp5N2B1gzarUjiOofgOn2b6DkY5a0bSbsCVwHERcU/10+0XOWs+CDha0lnAjsCzkp6OiNnV\nT7uftfokzda+AWfzwpPPZ9VpMw64l+KE89j0fFxNm0kMjZPtWeulOBd0ObBNq9fSxzqHUxz73ovn\nT8K+sqbNCbzwJOyP0vNX8sKT7SsZGifbc9a8Y2r/gVavY6DWXNPmNIbwyfaWT2Br3yiOD98ILE+P\nPW+Y7cD3S+0+TnHSdQUwo844QyVINnu9FH/tBcW3Znal7ZOtXlMva3038EeKq3q+nMq+Brw3Pd+e\n4mqdFcBvgZeX+n459VvGIL0yrT/XDJwK/LX0e+0Cdm71eqr+PZfGGNJB4lukmJlZFl+1ZWZmWRwk\nZmaWxUFiZmZZHCRmZpbFQWJmZlkcJDakSfp1epwk6Z/7eewv1Xutqkg6UtJXKhr7S3232uQxXy2p\no7/HtaHHl//aFkHSVIrr8N+zCX2GRcTGXuqfiIhR/TG/Jufza4rPHqzLHOdF66pqLel2OB+PiAf6\ne2wbOrxHYkOapCfS0zOBf5TUJelzkoZJOlvS7yTdJelTqf1USTdJugRYnMr+J30HxlJJM1PZmcBL\n0ngXl19LhbMlLUnfn/Hh0tg3S7osfWfKxaW7vJ4p6e40l2/WWccrgGd6QkRSh6Q5kn4p6Y+S3pPK\nm15Xaex6azlW0m9T2XmShvWsUdIZkhZJuk3ShFT+wbTeRZIWlIa/huLT2rY1a/UnIr15y9mAJ9Lj\nVODaUvlM4NT0fDugk+I2FlMpPkG9V6ltz6frXwIsAXYqj13ntT5AcWv7YcAE4AFgYhr7LxSfwN8G\n+A3wFopbvizj+SMAO9ZZxwzgnNLPHcDP0zj7UNyvaftNWVe9uafn+1MEwIj083cp7m8FxZ0DjkjP\nzyq91mJg19r5A28Grmn1fwfeWrv5po22pToEOFDS0ennMRRvyH8DfhvF93z0+D+SjkrPd0/tHu5l\n7LcA86M4fPSQpFuANwCPpbFXAUjqorh1zW3A08D3Jf0EuLbOmBOB7pqyH0XEs8BySSuB/TZxXY0c\nDLwe+F3aYXoJz98882+l+S0E3pWe3wp0SPoRcEVprLXALk28pm3BHCS2pRLwrxFx3QsKi3Mpf635\n+Z3AmyLiSUk3U/zl39fYjZS/Q2MjMDwiNkiaQvEGfgxwIlD7VbJPUYRCWe0JzJ5bzPe5rj4IuDAi\nTqlT9/eI6HndjaT3iIg4XtJBwD8BXZImR8TDFP9WTzX5uraF8jkS21I8Dowu/Xwd8GlJI6A4B5G+\nSKjWGOCRFCL7AW8s1f29p3+NBcCH0/mKNuCtFDfjq0vFt1iOiYifAp8FJtdp9nvgH2rKPihpG0l7\nU3yV67JNWFet8lpupLh9+c5pjHGS9uyts6S9I+L2iPgKsI7nb53+CorDgbYV8x6JbSnuAjZIWkRx\nfuHbFIeV7kgnvLup/7W+PweOl3QXxRv1baW6ucBdku6IiI+Uyq+k+C6YRRR7CV+MiD+nIKpnNHCV\npO0p9gY+V6fNAuAcSSrtESwDbqE4D3N8RDwt6ftNrqvWC9Yi6VTgeknbAH+nuNX5/b30P1vSPmn+\nN6a1A7wd+EkTr29bMF/+azZISPo2xYnrX6TPZ1wbEZe1eFoNqfhO9VuAt0TxFbK2lfKhLbPB4/8C\nI1s9iU2wB8WXlDlEtnLeIzEzsyzeIzEzsywOEjMzy+IgMTOzLA4SMzPL4iAxM7Ms/x/AxXmxjklL\nGAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3cf0a680b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "Train Accuracy: 0.666291\n",
      "Test Accuracy: 0.7016\n",
      "Termino\n",
      "CPU times: user 20 s, sys: 2.17 s, total: 22.2 s\n",
      "Wall time: 9.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parameters = model(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
