{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cargo\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "np.set_printoptions(edgeitems=3,infstr='inf',linewidth=75, nanstr='nan', precision=8,suppress=False, threshold=1000, formatter=None)\n",
    "np.random.seed(1)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print (\"cargo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def datos():\n",
    "    \n",
    "    print(\"Comenzo la carga...\")\n",
    "    train_req = pd.read_excel(\"train_requerimientos.xlsx\")\n",
    "    train_cliente = pd.read_excel(\"train_clientes.xlsx\")\n",
    "    test_cliente = pd.read_excel('test_clientes.xlsx')\n",
    "    test_req = pd.read_excel('test_requerimientos.xlsx')\n",
    "    #'CODMES'\n",
    "\n",
    "    \n",
    "    #Y_train = train_cliente['ATTRITION']\n",
    "    \n",
    "    group_train2 =  train_req.groupby([\"ID_CORRELATIVO\",\"TIPO_REQUERIMIENTO2\",\"DICTAMEN\"])\n",
    "    group_test2 =  test_req.groupby([\"ID_CORRELATIVO\",\"TIPO_REQUERIMIENTO2\",\"DICTAMEN\"])\n",
    "\n",
    "    df_train_req=group_train2.size().to_frame('size').reset_index()\n",
    "    df_test_req=group_test2.size().to_frame('size').reset_index()\n",
    "\n",
    "    df_train_req['FUSION']=df_train_req['TIPO_REQUERIMIENTO2']+\" \"+df_train_req['DICTAMEN']\n",
    "    df_test_req['FUSION']=df_train_req['TIPO_REQUERIMIENTO2']+\" \"+df_train_req['DICTAMEN']\n",
    "\n",
    "    pivot=df_train_req.pivot_table(values='size',index=\"ID_CORRELATIVO\", columns=[\"TIPO_REQUERIMIENTO2\",\"FUSION\"],aggfunc=np.sum)\n",
    "    pivot_test=df_test_req.pivot_table(values='size',index=\"ID_CORRELATIVO\", columns=[\"TIPO_REQUERIMIENTO2\",\"FUSION\"],aggfunc=np.sum)\n",
    "\n",
    "    pivot.fillna(value=0, inplace=True)\n",
    "    pivot_test.fillna(value=0, inplace=True)\n",
    "\n",
    "    pivot_total_reclamo= pivot[\"Reclamo\"].sum(axis=1)\n",
    "    pivot_total_reclamo_test= pivot_test[\"Reclamo\"].sum(axis=1)\n",
    "    #pivot_total_reclamo=pivot_total_reclamo.to_frame('SUMA RECLA')\n",
    "    pivot_total_solicitud= pivot[\"Solicitud\"].sum(axis=1)\n",
    "    pivot_total_solicitud_test= pivot_test[\"Solicitud\"].sum(axis=1)\n",
    "    #pivot_total_solicitud=pivot_total_solicitud.to_frame('SUMA SOLI')\n",
    "    #pivot.reset_index(inplace=True)\n",
    "    #pivot_total_reclamo.reset_index(inplace=True)\n",
    "    #pivot_total_reclamo\n",
    "\n",
    "    pivot_no_procede_reclamo=pivot[\"Reclamo\"][\"Reclamo NO PROCEDE\"]\n",
    "    pivot_reclamo_parcial=pivot[\"Reclamo\"][\"Reclamo PROCEDE PARCIAL\"]\n",
    "\n",
    "    pivot_no_procede_reclamo_test=pivot_test[\"Reclamo\"][\"Reclamo NO PROCEDE\"]\n",
    "    pivot_reclamo_parcial_test=pivot_test[\"Reclamo\"][\"Reclamo PROCEDE PARCIAL\"]\n",
    "    #pivot_no_procede_reclamo.to_frame('No Reclamo')\n",
    "    pivot_no_procede_solicitud=pivot[\"Solicitud\"][\"Solicitud NO PROCEDE\"]\n",
    "    pivot_solicitud_parcial=pivot[\"Solicitud\"][\"Solicitud PROCEDE PARCIAL\"]\n",
    "\n",
    "    pivot_no_procede_solicitud_test=pivot_test[\"Solicitud\"][\"Solicitud NO PROCEDE\"]\n",
    "    pivot_solicitud_parcial_test=pivot_test[\"Solicitud\"][\"Solicitud PROCEDE PARCIAL\"]\n",
    "\n",
    "\n",
    "    #pivot_no_procede_reclamo.to_frame('No Solicitud')\n",
    "\n",
    "    requeremiento={}\n",
    "    requeremiento_test={}\n",
    "    requerimiento={\"reclamos\":pivot_total_reclamo, \"no_reclamos\":pivot_no_procede_reclamo,\"reclamo_parcial\":pivot_reclamo_parcial ,\"solicitudes\":pivot_total_solicitud,\"no_solicitudes\":pivot_no_procede_solicitud,\"solicitudes_parcial\":pivot_solicitud_parcial}\n",
    "    requerimiento_test={\"reclamos\":pivot_total_reclamo_test, \"no_reclamos\":pivot_no_procede_reclamo_test ,\"reclamo_parcial\":pivot_reclamo_parcial_test,\"solicitudes\":pivot_total_solicitud_test,\"no_solicitudes\":pivot_no_procede_solicitud_test,\"solicitudes_parcial\":pivot_solicitud_parcial_test}\n",
    "\n",
    "\n",
    "    df_requerimiento=pd.DataFrame(requerimiento)\n",
    "    df_requerimiento_test=pd.DataFrame(requerimiento_test)\n",
    "\n",
    "    df_requerimiento[\"no_reclamos\"] = np.where(df_requerimiento[\"reclamos\"]==0, 0, df_requerimiento[\"no_reclamos\"]/df_requerimiento[\"reclamos\"])\n",
    "    df_requerimiento[\"reclamo_parcial\"] = np.where(df_requerimiento[\"reclamos\"]==0, 0, df_requerimiento[\"reclamo_parcial\"]/df_requerimiento[\"reclamos\"])\n",
    "\n",
    "    df_requerimiento_test[\"no_reclamos\"] = np.where(df_requerimiento_test[\"reclamos\"]==0, 0, df_requerimiento_test[\"no_reclamos\"]/df_requerimiento_test[\"reclamos\"])\n",
    "    df_requerimiento_test[\"reclamo_parcial\"] = np.where(df_requerimiento_test[\"reclamos\"]==0, 0, df_requerimiento_test[\"reclamo_parcial\"]/df_requerimiento_test[\"reclamos\"])\n",
    "\n",
    "    #procesado=pd.DataFrame(pivot_total_reclamo,pivot_total_solicitud)\n",
    "    df_requerimiento[\"no_solicitudes\"] = np.where(df_requerimiento[\"solicitudes\"]==0, 0, df_requerimiento[\"no_solicitudes\"]/df_requerimiento[\"solicitudes\"])\n",
    "    df_requerimiento[\"solicitudes_parcial\"] = np.where(df_requerimiento[\"solicitudes\"]==0, 0, df_requerimiento[\"solicitudes_parcial\"]/df_requerimiento[\"solicitudes\"])\n",
    "\n",
    "    df_requerimiento_test[\"no_solicitudes\"] = np.where(df_requerimiento_test[\"solicitudes\"]==0, 0, df_requerimiento_test[\"no_solicitudes\"]/df_requerimiento_test[\"solicitudes\"])\n",
    "    df_requerimiento_test[\"solicitudes_parcial\"] = np.where(df_requerimiento_test[\"solicitudes\"]==0, 0, df_requerimiento_test[\"solicitudes_parcial\"]/df_requerimiento_test[\"solicitudes\"])\n",
    "\n",
    "    df_requerimiento.reset_index(inplace=True)\n",
    "    df_requerimiento_test.reset_index(inplace=True)\n",
    "\n",
    "    train=pd.merge(train_cliente,df_requerimiento, on=\"ID_CORRELATIVO\", how='left')\n",
    "    test=pd.merge(test_cliente,df_requerimiento_test, on=\"ID_CORRELATIVO\", how='left')\n",
    "    \n",
    "    values = {'no_reclamos': 0, 'no_solicitudes': 0, 'reclamos': 0,'solicitudes': 0 ,'no_reclamos': 0,'no_solicitudes': 0,'reclamo_parcial':0,'solicitudes_parcial':0}\n",
    "    train.fillna(value=values, inplace=True)\n",
    "    test.fillna(value=values, inplace=True)\n",
    "    print (\"antes\")\n",
    "    print (train.shape)\n",
    "    ##ATTRITION\n",
    "    train.drop(['ID_CORRELATIVO','CODMES'], axis=1, inplace=True)\n",
    "    test.drop(['ID_CORRELATIVO','CODMES'], axis=1, inplace=True)\n",
    "    print (\"despues\")\n",
    "    print (train.shape)\n",
    "\n",
    "    train.dropna(inplace=True)\n",
    "    test.dropna(inplace=True)\n",
    "    \n",
    "    train= pd.get_dummies(train)\n",
    "    \n",
    "    #X_train, Y_train, X_test, Y_test\n",
    "    X_train=train.drop(['ATTRITION'], axis=1)\n",
    "    \n",
    "    Y_train_c=train[['ATTRITION']]\n",
    "    Y_train = Y_train_c.copy()\n",
    "    Y_train['NATTRITION']=1-Y_train['ATTRITION']   \n",
    "    \n",
    "    X_train_p, X_test_p, Y_train_p,Y_test_p = train_test_split(X_train,Y_train,train_size=0.85, random_state=0)\n",
    "    \n",
    "    X_train_p = X_train_p.as_matrix()\n",
    "    Y_train_p = Y_train_p.as_matrix()\n",
    "    X_test_p = X_test_p.as_matrix()\n",
    "    Y_test_p = Y_test_p.as_matrix()\n",
    "    ##normalizar datos de entrada\n",
    "    scaler= MinMaxScaler()\n",
    "    scaler.fit(X_train_p)\n",
    "    \n",
    "    X_train_p = scaler.transform(X_train_p)\n",
    "    X_test_p = scaler.transform(X_test_p)\n",
    "    \n",
    "    print (\"Termino cargar\")\n",
    "    return X_train_p.T, Y_train_p.T, X_test_p.T, Y_test_p.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargo Modelo\n"
     ]
    }
   ],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    X = tf.placeholder(tf.float32,shape=(n_x, None),name=\"X\")\n",
    "    Y = tf.placeholder(tf.float32,shape=(n_y, None),name=\"Y\")\n",
    "    ### Otras variables ###\n",
    "    \n",
    "     # variable learning rate\n",
    "    lr = tf.placeholder(tf.float32)\n",
    "    # train/test selector for batch normalisation\n",
    "    tst = tf.placeholder(tf.bool)\n",
    "    # training iteration\n",
    "    iter = tf.placeholder(tf.int32)   \n",
    "    # training iteration\n",
    "    pkeep = tf.placeholder(tf.float32)      \n",
    "    return X, Y, tst, iter,pkeep\n",
    "\n",
    "def initialize_parameters(layer,tipo=1):\n",
    "    parameters={}\n",
    "    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n",
    "    L=len(layer)\n",
    "    for i in range(1, L):\n",
    "        if(tipo==1):\n",
    "            parameters['W' + str(i)] =tf.get_variable('W' + str(i), [layer[i], layer[i-1]], initializer = tf.contrib.layers.xavier_initializer(seed = 1))        \n",
    "            parameters['b' + str(i)] =tf.get_variable('b' + str(i), [layer[i], 1], initializer = tf.zeros_initializer())\n",
    "        \n",
    "        if(tipo==2):\n",
    "            parameters['W' + str(i)] =tf.Variable(np.random.randn(layer[i], layer[i-1])*0.01,dtype=tf.float32,name='W' + str(i))        \n",
    "            parameters['b' + str(i)] =tf.get_variable('b' + str(i), [layer[i], 1], initializer = tf.zeros_initializer())\n",
    "            \n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def batchnorm(Ylogits, is_test,iter):\n",
    "    Ylogits = tf.transpose(Ylogits)\n",
    "    exp_moving_avg = tf.train.ExponentialMovingAverage(0.999,iter) # adding the iteration prevents from averaging across non-existing iterations\n",
    "    bnepsilon = 1e-5\n",
    "    \n",
    "    mean, variance = tf.nn.moments(Ylogits, [0])\n",
    "    \n",
    "    update_moving_averages = exp_moving_avg.apply([mean, variance])\n",
    "    m = tf.cond(is_test, lambda: exp_moving_avg.average(mean), lambda: mean)\n",
    "    v = tf.cond(is_test, lambda: exp_moving_avg.average(variance), lambda: variance)\n",
    "    \n",
    "    Ybn = tf.nn.batch_normalization(Ylogits, m, v, None, None, bnepsilon)\n",
    "    Ybn = tf.transpose(Ybn)\n",
    "    return Ybn, update_moving_averages\n",
    "\n",
    "def forward_propagation(X, parameters,tst,iter,pkeep):   \n",
    "    maximo= (len(parameters))\n",
    "    cont =1\n",
    "    l_update_ema=[]\n",
    "    for i in range(1,maximo+1,2):\n",
    "        if (cont==((maximo)/2)):\n",
    "            break\n",
    "        if(i==1):\n",
    "            Z=linear_forward(X,parameters['W'+str(cont)],parameters['b'+str(cont)])\n",
    "            #Z = tf.matmul(parameters['W'+str(cont)],X)\n",
    "            ZB, update_ema = batchnorm(Z, tst,iter)\n",
    "            A = tf.nn.relu(ZB)                                              \n",
    "            #Ad = sess.run(tf.nn.dropout(A, pkeep))\n",
    "            Ad = tf.nn.dropout(A, pkeep)\n",
    "        else:\n",
    "            #Z = tf.matmul(parameters['W'+str(cont)],Ad)\n",
    "            Z=linear_forward(Ad,parameters['W'+str(cont)],parameters['b'+str(cont)])\n",
    "            ZB, update_ema = batchnorm(Z, tst,iter)\n",
    "            A = tf.nn.relu(ZB)                                        \n",
    "            #Ad = tf.nn.dropout(A, pkeep)\n",
    "            Ad = tf.nn.dropout(A, pkeep)\n",
    "        l_update_ema.append(update_ema)\n",
    "        cont=cont+1\n",
    "            \n",
    "    #Z3 = tf.add(tf.matmul(parameters['W'+str(cont)],Ad),parameters['b'+str(cont)])    \n",
    "    Z3=linear_forward(Ad,parameters['W'+str(cont)],parameters['b'+str(cont)])\n",
    "    ##softmax\n",
    "    Y_Y = tf.nn.softmax(tf.transpose(Z3))\n",
    "    if((cont==3)):\n",
    "        update_ema = tf.group(l_update_ema[0],l_update_ema[1])\n",
    "    if((cont==4)):\n",
    "        update_ema = tf.group(l_update_ema[0],l_update_ema[1],l_update_ema[2])\n",
    "    if((cont==5)):\n",
    "        update_ema = tf.group(l_update_ema[0],l_update_ema[1],l_update_ema[2],l_update_ema[3])  \n",
    "    if((cont==6)):\n",
    "        update_ema = tf.group(l_update_ema[0],l_update_ema[1],l_update_ema[2],l_update_ema[3],l_update_ema[4])\n",
    "    if((cont==7)):\n",
    "        update_ema = tf.group(l_update_ema[0],l_update_ema[1],l_update_ema[2],l_update_ema[3],l_update_ema[4],l_update_ema[5])\n",
    "    if((cont==8)):\n",
    "        update_ema = tf.group(l_update_ema[0],l_update_ema[1],l_update_ema[2],l_update_ema[3],l_update_ema[4],l_update_ema[5],l_update_ema[6])\n",
    "    if((cont==9)):\n",
    "        update_ema = tf.group(l_update_ema[0],l_update_ema[1],l_update_ema[2],l_update_ema[3],l_update_ema[4],l_update_ema[5],l_update_ema[6],l_update_ema[7])\n",
    "    if((cont==10)):\n",
    "        update_ema = tf.group(l_update_ema[0],l_update_ema[1],l_update_ema[2],l_update_ema[3],l_update_ema[4],l_update_ema[5],l_update_ema[6],l_update_ema[7],l_update_ema[8])        \n",
    "    return Z3,update_ema,Y_Y\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    Z = tf.add(tf.matmul(W,A),b)  \n",
    "    return Z\n",
    "\n",
    "# GRADED FUNCTION: compute_cost \n",
    "def compute_cost(Z3, Y):\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, \n",
    "          learning_rate = 0.0001,\n",
    "          num_epochs = 30, \n",
    "          minibatch_size = 32, \n",
    "          print_cost = False,\n",
    "          pkeepv=1.0,\n",
    "         layer=[],\n",
    "         inicializa=1):\n",
    "\n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    cost_train = []                                        # To keep track of the cost\n",
    "    cost_test = []\n",
    "    \n",
    "    #print (n_x)\n",
    "    #print (m)\n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    X, Y,tst,iter,pkeep= create_placeholders(n_x, n_y)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    ### inicializar los parametros y la arquitectura de la red\n",
    "    #parameters = initialize_parameters(n_x,n_y,2)\n",
    "    parameters = initialize_parameters(layer,1)\n",
    "    #print (parameters)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    Z3,update_ema,Y_Y = forward_propagation(X, parameters,tst,iter,pkeep)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    cost = compute_cost(Z3,Y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    ##### Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        i=0\n",
    "        for epoch in range(num_epochs):\n",
    "            i=i+1\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "            #print (\"mini \"+str(num_minibatches))\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                #_ , minibatch_cost, _ = sess.run([optimizer, cost, update_ema], feed_dict={X: minibatch_X, Y: minibatch_Y, tst: False, iter: i,pkeep: pkeepv})\n",
    "                ### END CODE HERE ###\n",
    "                minibatch_cost = sess.run(cost, feed_dict={X: minibatch_X, Y: minibatch_Y, tst: False,pkeep:pkeepv})\n",
    "                sess.run(optimizer, feed_dict={X: minibatch_X, Y: minibatch_Y, tst: False, iter: i, pkeep:pkeepv})\n",
    "                sess.run(update_ema, feed_dict={X: minibatch_X, Y: minibatch_Y, tst: False, iter: i, pkeep:pkeepv})\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "                #if(i==30):\n",
    "                #    break\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 10 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            #if print_cost == True and epoch % 5 == 0:\n",
    "            #    cost_train.append(epoch_cost)\n",
    "            #    c_test = sess.run([ cost], feed_dict={X: minibatch_X, Y: minibatch_Y, tst: True,pkeep: 1.0})\n",
    "            #    cost_test.append(c_test)\n",
    "        '''\n",
    "        if(print_cost== True):\n",
    "            # plot the cost\n",
    "            plt.figure(\"Figura1\")\n",
    "            plt.plot(np.squeeze(cost_train))\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('iterations (per tens)')\n",
    "            plt.title(\"Train Learning rate =\" + str(learning_rate))\n",
    "\n",
    "            plt.figure(\"Figura2\")\n",
    "            plt.plot(np.squeeze(cost_test))\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('iterations (per tens)')\n",
    "            plt.title(\"Test Learning rate =\" + str(learning_rate))        \n",
    "\n",
    "\n",
    "            plt.figure(\"Figura3\")\n",
    "            plt.plot(np.squeeze(cost_train))\n",
    "            plt.plot(np.squeeze(cost_test))\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('iterations (per tens)')\n",
    "            plt.title(\"Test Learning rate =\" + str(learning_rate)) \n",
    "            plt.legend(loc=4)\n",
    "            plt.show()      \n",
    "          '''\n",
    "\n",
    "        \n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        #print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions, xq no recibe a3. Claro no lo tengo. Â¿como sabe que debe calcularlo?\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "        #z3_value = z3\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        acc_train=sess.run(accuracy, feed_dict ={X: X_train, Y: Y_train,tst: False,pkeep: 1.0})\n",
    "        acc_test, A3_value=sess.run([accuracy,Y_Y] ,feed_dict ={X: X_test, Y: Y_test,tst: True,pkeep: 1.0})\n",
    "        \n",
    "        #print (\"Train Accuracy :\", accuracy.eval({X: X_train, Y: Y_train,tst: False,pkeep: 1.0}))\n",
    "        #print (\"Test Accuracy: \", accuracy.eval({X: X_test, Y: Y_test,tst: True,pkeep: 1.0}))\n",
    "        '''\n",
    "        print (\"Train Accuracy:\", sess.run(accuracy, feed_dict ={X: X_train, Y: Y_train})\n",
    "\n",
    "        print (\"Test Accuracy:\", sess.run(accuracy, feed_dict ={X: X_test, Y: Y_test})\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #print (\"-----Fin prueba------\")\n",
    "        return parameters,acc_train,acc_test,A3_value\n",
    "print (\"Cargo Modelo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzo la carga...\n",
      "antes\n",
      "(70000, 59)\n",
      "despues\n",
      "(70000, 57)\n",
      "Termino cargar\n",
      "(84, 47822)\n",
      "(2, 47822)\n",
      "(84, 8440)\n",
      "(2, 8440)\n",
      "CPU times: user 41.4 s, sys: 0 ns, total: 41.4 s\n",
      "Wall time: 41.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, Y_train, X_test, Y_test = datos()\n",
    "print (X_train.shape)\n",
    "#Y_train=Y_train.reshape(Y_train.shape[0],1)\n",
    "print (Y_train.shape)\n",
    "print (X_test.shape)\n",
    "#Y_test=Y_test.reshape(Y_test.shape[0],1)\n",
    "print (Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquitectura:  84-25-12-2  Learning Rate:  0.0214739521572  Dropout  0.85  Epoch:  10\n",
      "Cost after epoch 0: 0.332009\n",
      "Accuracity Train:  0.878006 Accuracity Test:  0.883649\n",
      "----- ----- -----\n",
      "Arquitectura:  84-25-12-2  Learning Rate:  0.00131432275284  Dropout  0.85  Epoch:  10\n",
      "Cost after epoch 0: 0.348190\n",
      "Accuracity Train:  0.885931 Accuracity Test:  0.887204\n",
      "----- ----- -----\n",
      "Arquitectura:  84-25-12-2  Learning Rate:  0.998947123666  Dropout  0.85  Epoch:  10\n",
      "Cost after epoch 0: 0.591418\n",
      "Accuracity Train:  0.878006 Accuracity Test:  0.883649\n",
      "----- ----- -----\n",
      "Arquitectura:  84-25-12-2  Learning Rate:  0.0617546563725  Dropout  0.85  Epoch:  10\n",
      "Cost after epoch 0: 0.338222\n",
      "Accuracity Train:  0.878006 Accuracity Test:  0.883649\n",
      "----- ----- -----\n",
      "Arquitectura:  84-25-12-2  Learning Rate:  0.258807249177  Dropout  0.85  Epoch:  10\n",
      "Cost after epoch 0: 0.379488\n",
      "Accuracity Train:  0.878006 Accuracity Test:  0.883649\n",
      "----- ----- -----\n",
      "Arquitectura:  84-25-12-2  Learning Rate:  0.427214142282  Dropout  0.85  Epoch:  10\n",
      "Cost after epoch 0: 0.407392\n",
      "Accuracity Train:  0.878006 Accuracity Test:  0.883649\n",
      "----- ----- -----\n",
      "Arquitectura:  84-25-12-2  Learning Rate:  0.17987017388  Dropout  0.85  Epoch:  10\n",
      "Cost after epoch 0: 0.367783\n",
      "Accuracity Train:  0.878006 Accuracity Test:  0.883649\n",
      "----- ----- -----\n",
      "Arquitectura:  84-25-12-2  Learning Rate:  0.0414722016788  Dropout  0.85  Epoch:  10\n",
      "Cost after epoch 0: 0.336086\n",
      "Accuracity Train:  0.878006 Accuracity Test:  0.883649\n",
      "----- ----- -----\n",
      "Arquitectura:  84-25-12-2  Learning Rate:  0.0258779639234  Dropout  0.85  Epoch:  10\n",
      "Cost after epoch 0: 0.333345\n",
      "Accuracity Train:  0.878006 Accuracity Test:  0.883649\n",
      "----- ----- -----\n",
      "Arquitectura:  84-25-12-2  Learning Rate:  0.00699411975267  Dropout  0.85  Epoch:  10\n",
      "Cost after epoch 0: 0.330050\n",
      "Accuracity Train:  0.881979 Accuracity Test:  0.884123\n",
      "----- ----- -----\n",
      "Arquitectura:  84-100-50-25-2  Learning Rate:  0.0214739521572  Dropout  0.85  Epoch:  10\n",
      "Cost after epoch 0: 0.332329\n",
      "Accuracity Train:  0.885137 Accuracity Test:  0.888863\n",
      "----- ----- -----\n",
      "Arquitectura:  84-100-50-25-2  Learning Rate:  0.00131432275284  Dropout  0.85  Epoch:  10\n",
      "Cost after epoch 0: 0.353989\n",
      "Accuracity Train:  0.889005 Accuracity Test:  0.890047\n",
      "----- ----- -----\n",
      "Arquitectura:  84-100-50-25-2  Learning Rate:  0.998947123666  Dropout  0.85  Epoch:  10\n",
      "Cost after epoch 0: 0.818124\n",
      "Accuracity Train:  0.878006 Accuracity Test:  0.883649\n",
      "----- ----- -----\n",
      "Arquitectura:  84-100-50-25-2  Learning Rate:  0.0617546563725  Dropout  0.85  Epoch:  10\n",
      "Cost after epoch 0: 0.345303\n",
      "Accuracity Train:  0.878194 Accuracity Test:  0.883649\n",
      "----- ----- -----\n",
      "Arquitectura:  84-100-50-25-2  Learning Rate:  0.258807249177  Dropout  0.85  Epoch:  10\n",
      "Cost after epoch 0: 0.439003\n",
      "Accuracity Train:  0.878006 Accuracity Test:  0.883649\n",
      "----- ----- -----\n",
      "Arquitectura:  84-100-50-25-2  Learning Rate:  0.427214142282  Dropout  0.85  Epoch:  10\n",
      "Cost after epoch 0: 0.539853\n",
      "Accuracity Train:  0.878006 Accuracity Test:  0.883649\n",
      "----- ----- -----\n",
      "Arquitectura:  84-100-50-25-2  Learning Rate:  0.17987017388  Dropout  0.85  Epoch:  10\n",
      "Cost after epoch 0: 0.407966\n",
      "Accuracity Train:  0.878006 Accuracity Test:  0.883649\n",
      "----- ----- -----\n",
      "Arquitectura:  84-100-50-25-2  Learning Rate:  0.0414722016788  Dropout  0.85  Epoch:  10\n",
      "Cost after epoch 0: 0.338835\n",
      "Accuracity Train:  0.880557 Accuracity Test:  0.88436\n",
      "----- ----- -----\n",
      "Arquitectura:  84-100-50-25-2  Learning Rate:  0.0258779639234  Dropout  0.85  Epoch:  10\n",
      "Cost after epoch 0: 0.332395\n",
      "Accuracity Train:  0.886768 Accuracity Test:  0.88981\n",
      "----- ----- -----\n",
      "Arquitectura:  84-100-50-25-2  Learning Rate:  0.00699411975267  Dropout  0.85  Epoch:  10\n",
      "Cost after epoch 0: 0.329481\n",
      "Accuracity Train:  0.887186 Accuracity Test:  0.88827\n",
      "----- ----- -----\n",
      "----Termino-----\n",
      "CPU times: user 38min 51s, sys: 3min 1s, total: 41min 53s\n",
      "Wall time: 18min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(1)\n",
    "#a_learning_rate = [0.0001, 0.009, 0.005, 0.001, 0.09, 0.05, 0.01, 0.9, 0.5, 0.1]\n",
    "#a_pkeepv = [1,0.95,0.90,0.85,0.80]\n",
    "(n_x, m) = X_train.shape                         \n",
    "n_y = Y_train.shape[0] \n",
    "l_layer=[]\n",
    "#l_layer.append([n_x,25,12,n_y])\n",
    "#l_layer.append([n_x,100,50,25,n_y])\n",
    "#l_layer.append([n_x,200,100,50,25,n_y])\n",
    "l_layer.append([n_x,1000,500,n_y])\n",
    "#l_layer.append([n_x,1000,200,25,n_y])\n",
    "#l_layer.append([n_x,400,200,100,50,25,n_y])\n",
    "#l_layer.append([n_x,800,400,200,100,50,25,n_y])\n",
    "l_layer.append([n_x,1600,800,400,200,100,50,25,n_y])\n",
    "### E_learning_rate\n",
    "\n",
    "l_el = np.random.uniform(-4,-2,10)\n",
    "a_learning_rate=np.unique(np.around(np.power(10,l_el),decimals=5))\n",
    "#l_el=-4*np.random.rand(10)\n",
    "\n",
    "#a_learning_rate=np.unique(np.around(np.power(10,l_el),decimals=3))\n",
    "##a_learning_rate = [0.0001]\n",
    "inicializa=2\n",
    "#a_pkeepv = [1,0.85],no generalize al inicio\n",
    "a_pkeepv = [1]\n",
    "nume=50\n",
    "result=[]\n",
    "for e_layer in l_layer:\n",
    "    for e_learning_rate in a_learning_rate:\n",
    "        for e_pkeepv in a_pkeepv:\n",
    "            print (\"Arquitectura: \",\"-\".join(map(str, e_layer)),\" Learning Rate: \", e_learning_rate, \" Dropout \", e_pkeepv,\" Epoch: \", nume)\n",
    "            parameters,acc_train,acc_test,A3_value = model(X_train, \n",
    "                                                  Y_train, \n",
    "                                                  X_test, \n",
    "                                                  Y_test,\n",
    "                                                  learning_rate=e_learning_rate,\n",
    "                                                  pkeepv=e_pkeepv,\n",
    "                                                  num_epochs=nume,\n",
    "                                                  print_cost=True,\n",
    "                                                  layer=e_layer,\n",
    "                                                inicializa=2)\n",
    "            result.append([e_layer,e_learning_rate,e_pkeepv,acc_train,acc_test,nume,inicializa])\n",
    "            print (\"Accuracity Train: \", acc_train,\"Accuracity Test: \" ,acc_test)\n",
    "            print(\"----- ----- -----\")\n",
    "\n",
    "fecha=time.strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "resultado = '\\n'.join(map(str, result))\n",
    "\n",
    "f = open ('resultado/archivo'+fecha+'.txt',\"w\")\n",
    "f.write(resultado)\n",
    "f.close()\n",
    "print(\"----Termino-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02828412  0.97171581]\n",
      " [ 0.04219292  0.95780712]\n",
      " [ 0.04681063  0.95318937]\n",
      " [ 0.05038058  0.94961935]\n",
      " [ 0.39675713  0.60324293]\n",
      " [ 0.1171367   0.88286334]\n",
      " [ 0.09787513  0.90212482]\n",
      " [ 0.06176955  0.93823045]\n",
      " [ 0.15814407  0.84185594]\n",
      " [ 0.06751215  0.93248779]]\n"
     ]
    }
   ],
   "source": [
    "print (A3_value[0:10,0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultado = '\\n'.join(map(str, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A3_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "aresu=np.asarray(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[list([84, 25, 12, 2]), 0.021473952157175989, 0.85, 0.87800592,\n",
       "        0.88364929],\n",
       "       [list([84, 25, 12, 2]), 0.001314322752836385, 0.85, 0.88593113,\n",
       "        0.88720381],\n",
       "       [list([84, 25, 12, 2]), 0.99894712366592076, 0.85, 0.87800592,\n",
       "        0.88364929],\n",
       "       [list([84, 25, 12, 2]), 0.061754656372527347, 0.85, 0.87800592,\n",
       "        0.88364929],\n",
       "       [list([84, 25, 12, 2]), 0.25880724917679454, 0.85, 0.87800592,\n",
       "        0.88364929],\n",
       "       [list([84, 25, 12, 2]), 0.42721414228233884, 0.85, 0.87800592,\n",
       "        0.88364929],\n",
       "       [list([84, 25, 12, 2]), 0.17987017387999535, 0.85, 0.87800592,\n",
       "        0.88364929],\n",
       "       [list([84, 25, 12, 2]), 0.041472201678756003, 0.85, 0.87800592,\n",
       "        0.88364929],\n",
       "       [list([84, 25, 12, 2]), 0.025877963923366044, 0.85, 0.87800592,\n",
       "        0.88364929],\n",
       "       [list([84, 25, 12, 2]), 0.0069941197526672593, 0.85, 0.88197899,\n",
       "        0.88412321],\n",
       "       [list([84, 100, 50, 25, 2]), 0.021473952157175989, 0.85, 0.88513654,\n",
       "        0.88886255],\n",
       "       [list([84, 100, 50, 25, 2]), 0.001314322752836385, 0.85, 0.88900506,\n",
       "        0.89004737],\n",
       "       [list([84, 100, 50, 25, 2]), 0.99894712366592076, 0.85, 0.87800592,\n",
       "        0.88364929],\n",
       "       [list([84, 100, 50, 25, 2]), 0.061754656372527347, 0.85, 0.87819415,\n",
       "        0.88364929],\n",
       "       [list([84, 100, 50, 25, 2]), 0.25880724917679454, 0.85, 0.87800592,\n",
       "        0.88364929],\n",
       "       [list([84, 100, 50, 25, 2]), 0.42721414228233884, 0.85, 0.87800592,\n",
       "        0.88364929],\n",
       "       [list([84, 100, 50, 25, 2]), 0.17987017387999535, 0.85, 0.87800592,\n",
       "        0.88364929],\n",
       "       [list([84, 100, 50, 25, 2]), 0.041472201678756003, 0.85, 0.88055706,\n",
       "        0.88436019],\n",
       "       [list([84, 100, 50, 25, 2]), 0.025877963923366044, 0.85, 0.88676763,\n",
       "        0.88981044],\n",
       "       [list([84, 100, 50, 25, 2]), 0.0069941197526672593, 0.85,\n",
       "        0.88718581, 0.88827014]], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aresu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
