{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cargo\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "np.random.seed(1)\n",
    "print (\"cargo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def datos():\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    mnist = input_data.read_data_sets('/tmp/tensorflow/mnist/input_data', one_hot=True)\n",
    "    print (\"Dimensiones train\")\n",
    "    print (mnist.train.images.shape)\n",
    "    print (\"Dimensiones test\")\n",
    "    print (mnist.test.images.shape)\n",
    "    print (\"cargo\")\n",
    "    X_train_o = mnist.train.images[:55000,:]\n",
    "    Y_train_o = mnist.train.labels[:55000,:]\n",
    "    X_test_o = mnist.test.images[:10000,:]\n",
    "    Y_test_o = mnist.test.labels[:10000,:]\n",
    "    print (\"Dimensiones X train\")\n",
    "    print (X_train_o.shape)\n",
    "    print (\"Dimensiones Y train\")\n",
    "    print (Y_train_o.shape)\n",
    "    print (\"Las dimensiones de X son Nx por M, donde Nx son las caracteristicas y M es el numero, por eso hay que trasponerlo\")\n",
    "    X_train=X_train_o.T\n",
    "    Y_train=Y_train_o.T\n",
    "    print (\"........\")\n",
    "    print (\"X train transpuesto\", str(X_train.shape))\n",
    "    print (\"Y train transpuesto\", str(Y_train.shape))\n",
    "    X_test=X_test_o.T\n",
    "    Y_test=Y_test_o.T\n",
    "    print (\"X test transpuesto\", str(X_test.shape))\n",
    "    print (\"Y test transpuesto\", str(Y_test.shape))\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    X = tf.placeholder(tf.float32,shape=(n_x, None),name=\"X\")\n",
    "    Y = tf.placeholder(tf.float32,shape=(n_y, None),name=\"Y\")\n",
    "    ### Otras variables ###\n",
    "    \n",
    "     # variable learning rate\n",
    "    lr = tf.placeholder(tf.float32)\n",
    "    # train/test selector for batch normalisation\n",
    "    tst = tf.placeholder(tf.bool)\n",
    "    # training iteration\n",
    "    iter = tf.placeholder(tf.int32)   \n",
    "    # training iteration\n",
    "    pkeep = tf.placeholder(tf.float32)      \n",
    "    return X, Y, tst, iter,pkeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x,n_y,tipo=1):\n",
    "    \n",
    "    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n",
    "        \n",
    "    ### START CODE HERE ### (approx. 6 lines of code)\n",
    "    if(tipo==1):\n",
    "        W1 =  tf.get_variable(\"W1\", [25,n_x], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "        b1 = tf.get_variable(\"b1\", [25,1], initializer = tf.zeros_initializer())\n",
    "        W2 =  tf.get_variable(\"W2\", [12,25], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "        b2 = tf.get_variable(\"b2\", [12,1], initializer = tf.zeros_initializer())\n",
    "        W3 =  tf.get_variable(\"W3\", [n_y,12], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "        b3 = tf.get_variable(\"b3\", [n_y,1], initializer = tf.zeros_initializer())\n",
    "    elif(tipo==2):\n",
    "       \n",
    "        W1 =  tf.Variable(np.random.randn(25, n_x)*0.01,dtype=tf.float32,name=\"W1\")\n",
    "        b1 = tf.get_variable(\"b1\", [25,1], initializer = tf.zeros_initializer())\n",
    "        W2 =  tf.Variable(np.random.randn(12, 25)*0.01,dtype=tf.float32,name=\"W2\")\n",
    "        b2 = tf.get_variable(\"b2\", [12,1], initializer = tf.zeros_initializer())\n",
    "        W3 =  tf.Variable(np.random.randn(n_y, 12)*0.01,dtype=tf.float32,name=\"W3\")\n",
    "        b3 = tf.get_variable(\"b3\", [n_y,1], initializer = tf.zeros_initializer())       \n",
    "        \n",
    "        \n",
    "        \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchnorm(Ylogits, is_test,iter):\n",
    "    #Ylogits = tf.transpose(Ylogits)\n",
    "    exp_moving_avg = tf.train.ExponentialMovingAverage(0.999,iter) # adding the iteration prevents from averaging across non-existing iterations\n",
    "    bnepsilon = 1e-5\n",
    "    \n",
    "    mean, variance = tf.nn.moments(Ylogits, [0])\n",
    "    \n",
    "    update_moving_averages = exp_moving_avg.apply([mean, variance])\n",
    "    m = tf.cond(is_test, lambda: exp_moving_avg.average(mean), lambda: mean)\n",
    "    v = tf.cond(is_test, lambda: exp_moving_avg.average(variance), lambda: variance)\n",
    "    \n",
    "    Ybn = tf.nn.batch_normalization(Ylogits, m, v, None, None, bnepsilon)\n",
    "    #Ybn = tf.transpose(Ybn)\n",
    "    return Ybn, update_moving_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters,tst,iter,pkeep):   \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    ### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:\n",
    "    #Z1 = tf.add(tf.matmul(W1,X),b1)  \n",
    "    Z1 = tf.transpose(tf.matmul(W1,X))                                              # Z1 = np.dot(W1, X) + b1\n",
    "    Z1B, update_ema1 = batchnorm(Z1, tst,iter)  \n",
    "    A1 = tf.nn.relu(Z1B)                                              # A1 = relu(Z1)\n",
    "    A1d = tf.nn.dropout(A1, pkeep)\n",
    "    A1d=tf.transpose(A1d)\n",
    "    #Z2 = tf.add(tf.matmul(W2,A1),b2)\n",
    "    \n",
    "    Z2 = tf.transpose(tf.matmul(W2,A1d))                                               # Z2 = np.dot(W2, a1) + b2\n",
    "    Z2B, update_ema2 = batchnorm(Z2, tst,iter)    \n",
    "    A2 = tf.nn.relu(Z2B)                                              # A2 = relu(Z2)\n",
    "    A2d = tf.nn.dropout(A2, pkeep)\n",
    "    A2d=tf.transpose(A2d)\n",
    "    \n",
    "    Z3 = tf.add(tf.matmul(W3,A2d),b3)                                             # Z3 = np.dot(W3,Z2) + b3\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    update_ema = tf.group(update_ema1,update_ema2)\n",
    "    return Z3,update_ema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost \n",
    "def compute_cost(Z3, Y):\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 30, minibatch_size = 32, print_cost = False,pkeepv=1.0):\n",
    "\n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    cost_train = []                                        # To keep track of the cost\n",
    "    cost_test = []\n",
    "    #print (n_x)\n",
    "    #print (m)\n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    X, Y,tst,iter,pkeep= create_placeholders(n_x, n_y)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    ### inicializar los parametros y la arquitectura de la red\n",
    "    parameters = initialize_parameters(n_x,n_y,2)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    Z3,update_ema = forward_propagation(X, parameters,tst,iter,pkeep)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    cost = compute_cost(Z3,Y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    ##### Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        i=0\n",
    "        for epoch in range(num_epochs):\n",
    "            i=i+1\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "            #print (\"mini \"+str(num_minibatches))\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                _ , minibatch_cost, _ = sess.run([optimizer, cost, update_ema], feed_dict={X: minibatch_X, Y: minibatch_Y, tst: False, iter: i,pkeep: pkeepv})\n",
    "                ### END CODE HERE ###\n",
    "                #minibatch_cost = sess.run(cost, feed_dict={X: minibatch_X, Y: minibatch_Y, tst: False})\n",
    "                #sess.run(optimizer, {X: minibatch_X, Y: minibatch_Y, tst: False, iter: i})\n",
    "                #sess.run(update_ema, {X: minibatch_X, Y: minibatch_Y, tst: False, iter: i})\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "                #if(i==30):\n",
    "                #    break\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                cost_train.append(epoch_cost)\n",
    "                c_test = sess.run([ cost], feed_dict={X: minibatch_X, Y: minibatch_Y, tst: True,pkeep: 1.0})\n",
    "                cost_test.append(c_test)\n",
    "  \n",
    "        if(print_cost== True):\n",
    "            # plot the cost\n",
    "            plt.figure(\"Figura1\")\n",
    "            plt.plot(np.squeeze(cost_train))\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('iterations (per tens)')\n",
    "            plt.title(\"Train Learning rate =\" + str(learning_rate))\n",
    "\n",
    "            plt.figure(\"Figura2\")\n",
    "            plt.plot(np.squeeze(cost_test))\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('iterations (per tens)')\n",
    "            plt.title(\"Test Learning rate =\" + str(learning_rate))        \n",
    "\n",
    "\n",
    "            plt.figure(\"Figura3\")\n",
    "            plt.plot(np.squeeze(cost_train))\n",
    "            plt.plot(np.squeeze(cost_test))\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('iterations (per tens)')\n",
    "            plt.title(\"Test Learning rate =\" + str(learning_rate)) \n",
    "            plt.legend(loc=4)\n",
    "            plt.show()\n",
    "        \n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions, xq no recibe a3. Claro no lo tengo. Â¿como sabe que debe calcularlo?\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        acc_train=sess.run(accuracy, feed_dict ={X: X_train, Y: Y_train,tst: False,pkeep: 1.0})\n",
    "        acc_test=sess.run(accuracy ,feed_dict ={X: X_test, Y: Y_test,tst: True,pkeep: 1.0})\n",
    "        \n",
    "        #print (\"Train Accuracy :\", accuracy.eval({X: X_train, Y: Y_train,tst: False,pkeep: 1.0}))\n",
    "        #print (\"Test Accuracy: \", accuracy.eval({X: X_test, Y: Y_test,tst: True,pkeep: 1.0}))\n",
    "        '''\n",
    "        print (\"Train Accuracy:\", sess.run(accuracy, feed_dict ={X: X_train, Y: Y_train})\n",
    "\n",
    "        print (\"Test Accuracy:\", sess.run(accuracy, feed_dict ={X: X_test, Y: Y_test})\n",
    "        \n",
    "        '''\n",
    "        print (\"-----Termino------\")\n",
    "        return parameters,acc_train,acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n",
      "Dimensiones train\n",
      "(55000, 784)\n",
      "Dimensiones test\n",
      "(10000, 784)\n",
      "cargo\n",
      "Dimensiones X train\n",
      "(55000, 784)\n",
      "Dimensiones Y train\n",
      "(55000, 10)\n",
      "Las dimensiones de X son Nx por M, donde Nx son las caracteristicas y M es el numero, por eso hay que trasponerlo\n",
      "........\n",
      "X train transpuesto (784, 55000)\n",
      "Y train transpuesto (10, 55000)\n",
      "X test transpuesto (784, 10000)\n",
      "Y test transpuesto (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = datos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001 1\n",
      "Parameters have been trained!\n",
      "Train Accuracy : 0.846855\n",
      "Test Accuracy:  0.8176\n",
      "Termino\n",
      "0.0001 0.95\n",
      "Parameters have been trained!\n",
      "Train Accuracy : 0.834545\n",
      "Test Accuracy:  0.8164\n",
      "Termino\n",
      "0.0001 0.9\n",
      "Parameters have been trained!\n",
      "Train Accuracy : 0.824382\n",
      "Test Accuracy:  0.8055\n",
      "Termino\n",
      "0.0001 0.85\n",
      "Parameters have been trained!\n",
      "Train Accuracy : 0.837673\n",
      "Test Accuracy:  0.8254\n",
      "Termino\n",
      "0.0001 0.8\n",
      "Parameters have been trained!\n",
      "Train Accuracy : 0.813\n",
      "Test Accuracy:  0.8002\n",
      "Termino\n",
      "0.009 1\n",
      "Parameters have been trained!\n",
      "Train Accuracy : 0.9638\n",
      "Test Accuracy:  0.9461\n",
      "Termino\n",
      "0.009 0.95\n",
      "Parameters have been trained!\n",
      "Train Accuracy : 0.953891\n",
      "Test Accuracy:  0.9414\n",
      "Termino\n",
      "0.009 0.9\n",
      "Parameters have been trained!\n",
      "Train Accuracy : 0.9484\n",
      "Test Accuracy:  0.9368\n",
      "Termino\n",
      "0.009 0.85\n",
      "Parameters have been trained!\n",
      "Train Accuracy : 0.939255\n",
      "Test Accuracy:  0.9321\n",
      "Termino\n",
      "0.009 0.8\n",
      "Parameters have been trained!\n",
      "Train Accuracy : 0.933309\n",
      "Test Accuracy:  0.9285\n",
      "Termino\n",
      "0.005 1\n",
      "Parameters have been trained!\n",
      "Train Accuracy : 0.964145\n",
      "Test Accuracy:  0.9455\n",
      "Termino\n",
      "0.005 0.95\n",
      "Parameters have been trained!\n",
      "Train Accuracy : 0.955727\n",
      "Test Accuracy:  0.9419\n",
      "Termino\n",
      "0.005 0.9\n",
      "Parameters have been trained!\n",
      "Train Accuracy : 0.947964\n",
      "Test Accuracy:  0.9364\n",
      "Termino\n",
      "0.005 0.85\n",
      "Parameters have been trained!\n",
      "Train Accuracy : 0.939891\n",
      "Test Accuracy:  0.9324\n",
      "Termino\n",
      "0.005 0.8\n",
      "Parameters have been trained!\n",
      "Train Accuracy : 0.928818\n",
      "Test Accuracy:  0.9223\n",
      "Termino\n",
      "0.001 1\n",
      "Parameters have been trained!\n",
      "Train Accuracy : 0.959436\n",
      "Test Accuracy:  0.9456\n",
      "Termino\n",
      "0.001 0.95\n",
      "Parameters have been trained!\n",
      "Train Accuracy : 0.952545\n",
      "Test Accuracy:  0.936\n",
      "Termino\n",
      "0.001 0.9\n",
      "Parameters have been trained!\n",
      "Train Accuracy : 0.945218\n",
      "Test Accuracy:  0.9361\n",
      "Termino\n",
      "0.001 0.85\n",
      "Parameters have been trained!\n",
      "Train Accuracy : 0.9344\n",
      "Test Accuracy:  0.9236\n",
      "Termino\n",
      "0.001 0.8\n",
      "Parameters have been trained!\n",
      "Train Accuracy : 0.930109\n",
      "Test Accuracy:  0.9204\n",
      "Termino\n",
      "0.09 1\n",
      "Parameters have been trained!\n",
      "Train Accuracy : 0.957055\n",
      "Test Accuracy:  0.943\n",
      "Termino\n",
      "0.09 0.95\n",
      "Parameters have been trained!\n",
      "Train Accuracy : 0.946564\n",
      "Test Accuracy:  0.9268\n",
      "Termino\n",
      "0.09 0.9\n",
      "Parameters have been trained!\n",
      "Train Accuracy : 0.943691\n",
      "Test Accuracy:  0.9371\n",
      "Termino\n",
      "0.09 0.85\n",
      "Parameters have been trained!\n",
      "Train Accuracy : 0.936255\n",
      "Test Accuracy:  0.9272\n",
      "Termino\n",
      "0.09 0.8\n",
      "Parameters have been trained!\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a_learning_rate = [0.0001, 0.009, 0.005, 0.001, 0.09, 0.05, 0.01, 0.9, 0.5, 0.1]\n",
    "a_pkeepv = [1,0.95,0.90,0.85,0.80]\n",
    "result=[]\n",
    "for e_learning_rate in a_learning_rate:\n",
    "    for e_pkeepv in a_pkeepv:\n",
    "        print (e_learning_rate , e_pkeepv)\n",
    "        parameters,acc_train,acc_test = model(X_train, Y_train, X_test, Y_test,learning_rate=e_learning_rate,pkeepv=e_pkeepv,num_epochs=2,print_cost=False)\n",
    "        result.append([e_learning_rate,e_pkeepv,acc_train,acc_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola\n"
     ]
    }
   ],
   "source": [
    "print (\"hola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
