{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cargo\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "np.random.seed(1)\n",
    "print (\"cargo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def datos():\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    mnist = input_data.read_data_sets('/tmp/tensorflow/mnist/input_data', one_hot=True)\n",
    "    print (\"Dimensiones train\")\n",
    "    print (mnist.train.images.shape)\n",
    "    print (\"Dimensiones test\")\n",
    "    print (mnist.test.images.shape)\n",
    "    print (\"cargo\")\n",
    "    X_train_o = mnist.train.images[:55000,:]\n",
    "    Y_train_o = mnist.train.labels[:55000,:]\n",
    "    X_test_o = mnist.test.images[:10000,:]\n",
    "    Y_test_o = mnist.test.labels[:10000,:]\n",
    "    print (\"Dimensiones X train\")\n",
    "    print (X_train_o.shape)\n",
    "    print (\"Dimensiones Y train\")\n",
    "    print (Y_train_o.shape)\n",
    "    print (\"Las dimensiones de X son Nx por M, donde Nx son las caracteristicas y M es el numero, por eso hay que trasponerlo\")\n",
    "    X_train=X_train_o.T\n",
    "    Y_train=Y_train_o.T\n",
    "    print (\"........\")\n",
    "    print (\"X train transpuesto\", str(X_train.shape))\n",
    "    print (\"Y train transpuesto\", str(Y_train.shape))\n",
    "    X_test=X_test_o.T\n",
    "    Y_test=Y_test_o.T\n",
    "    print (\"X test transpuesto\", str(X_test.shape))\n",
    "    print (\"Y test transpuesto\", str(Y_test.shape))\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    X = tf.placeholder(tf.float32,shape=(n_x, None),name=\"X\")\n",
    "    Y = tf.placeholder(tf.float32,shape=(n_y, None),name=\"Y\")\n",
    "    ### Otras variables ###\n",
    "    \n",
    "     # variable learning rate\n",
    "    lr = tf.placeholder(tf.float32)\n",
    "    # train/test selector for batch normalisation\n",
    "    tst = tf.placeholder(tf.bool)\n",
    "    # training iteration\n",
    "    #iter = tf.placeholder(tf.int32)   \n",
    "    \n",
    "    return X, Y, tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x,n_y):\n",
    "    \n",
    "    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n",
    "        \n",
    "    ### START CODE HERE ### (approx. 6 lines of code)\n",
    "    W1 =  tf.get_variable(\"W1\", [25,n_x], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b1 = tf.get_variable(\"b1\", [25,1], initializer = tf.zeros_initializer())\n",
    "    W2 =  tf.get_variable(\"W2\", [12,25], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b2 = tf.get_variable(\"b2\", [12,1], initializer = tf.zeros_initializer())\n",
    "    W3 =  tf.get_variable(\"W3\", [n_y,12], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b3 = tf.get_variable(\"b3\", [n_y,1], initializer = tf.zeros_initializer())\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchnorm(Ylogits, is_test):\n",
    "    Ylogits = tf.transpose(Ylogits)\n",
    "    exp_moving_avg = tf.train.ExponentialMovingAverage(0.999) # adding the iteration prevents from averaging across non-existing iterations\n",
    "    bnepsilon = 1e-5\n",
    "    \n",
    "    mean, variance = tf.nn.moments(Ylogits, [0])\n",
    "    \n",
    "    update_moving_averages = exp_moving_avg.apply([mean, variance])\n",
    "    m = tf.cond(is_test, lambda: exp_moving_avg.average(mean), lambda: mean)\n",
    "    v = tf.cond(is_test, lambda: exp_moving_avg.average(variance), lambda: variance)\n",
    "    \n",
    "    Ybn = tf.nn.batch_normalization(Ylogits, m, v, None, None, bnepsilon)\n",
    "    Ybn = tf.transpose(Ybn)\n",
    "    return Ybn, update_moving_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters,tst):   \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    ### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:\n",
    "    #Z1 = tf.add(tf.matmul(W1,X),b1)  \n",
    "    Z1 = tf.matmul(W1,X)                                              # Z1 = np.dot(W1, X) + b1\n",
    "    Z1B, update_ema1 = batchnorm(Z1, tst)\n",
    "    \n",
    "    A1 = tf.nn.relu(Z1B)                                              # A1 = relu(Z1)\n",
    "    \n",
    "    #Z2 = tf.add(tf.matmul(W2,A1),b2)\n",
    "    \n",
    "    Z2 = tf.matmul(W2,A1)                                               # Z2 = np.dot(W2, a1) + b2\n",
    "    Z2B, update_ema2 = batchnorm(Z2, tst)\n",
    "    \n",
    "    A2 = tf.nn.relu(Z2B)                                              # A2 = relu(Z2)\n",
    "    \n",
    "    \n",
    "    Z3 = tf.add(tf.matmul(W3,A2),b3)                                             # Z3 = np.dot(W3,Z2) + b3\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    #update_ema = tf.group(update_ema1)\n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost \n",
    "def compute_cost(Z3, Y):\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 300, minibatch_size = 32, print_cost = True):\n",
    "\n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    #print (n_x)\n",
    "    #print (m)\n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    X, Y,tst= create_placeholders(n_x, n_y)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    ### inicializar los parametros y la arquitectura de la red\n",
    "    parameters = initialize_parameters(n_x,n_y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    Z3 = forward_propagation(X, parameters,tst)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    cost = compute_cost(Z3,Y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    ##### Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        i=0\n",
    "        for epoch in range(num_epochs):\n",
    "            i=i+1\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "            #print (\"mini \"+str(num_minibatches))\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y, tst: False})\n",
    "                ### END CODE HERE ###\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "                #if(i==30):\n",
    "                #    break\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "  \n",
    "\n",
    "        # plot the cost\n",
    "        \n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions, xq no recibe a3. Claro no lo tengo. ¿como sabe que debe calcularlo?\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train,tst: False}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test,tst: True}))\n",
    "        '''\n",
    "        print (\"Train Accuracy:\", sess.run(accuracy, feed_dict ={X: X_train, Y: Y_train})\n",
    "\n",
    "        print (\"Test Accuracy:\", sess.run(accuracy, feed_dict ={X: X_test, Y: Y_test})\n",
    "        \n",
    "        '''\n",
    "        print (\"Termino\")\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/tensorflow/mnist/input_data\\train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data\\t10k-labels-idx1-ubyte.gz\n",
      "Dimensiones train\n",
      "(55000, 784)\n",
      "Dimensiones test\n",
      "(10000, 784)\n",
      "cargo\n",
      "Dimensiones X train\n",
      "(55000, 784)\n",
      "Dimensiones Y train\n",
      "(55000, 10)\n",
      "Las dimensiones de X son Nx por M, donde Nx son las caracteristicas y M es el numero, por eso hay que trasponerlo\n",
      "........\n",
      "X train transpuesto (784, 55000)\n",
      "Y train transpuesto (10, 55000)\n",
      "X test transpuesto (784, 10000)\n",
      "Y test transpuesto (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = datos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 1.549897\n",
      "Cost after epoch 100: 0.103043\n",
      "Cost after epoch 200: 0.080872\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xce81db67b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "Train Accuracy: 0.994327\n",
      "Test Accuracy: 0.9625\n",
      "Termino\n"
     ]
    }
   ],
   "source": [
    "%%Time\n",
    "parameters = model(X_train, Y_train, X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
